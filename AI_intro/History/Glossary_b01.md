<!-- Path: 114A_AI_intro/History | Timestamp: 2025-10-26 10:28:00 | Version: b01 -->
# 關鍵術語詞彙表 (Glossary)

[返回目錄](./README_b02.md)

---

本詞彙表整理了課程中出現的核心術語，以便快速查閱。

## A

- **AGI (Artificial General Intelligence / 通用人工智慧)**
  具備與人類同等智慧，能理解、學習並應用其智慧以解決任何問題的機器智慧，是 AI 領域的長期目標。

- **AlexNet**
  一個深度卷積神經網路 (CNN)，在 2012 年的 ImageNet 競賽中取得壓倒性勝利，標誌著深度學習時代的開啟。

- **Attention Mechanism (注意力機制)**
  一種模仿人類認知注意力的機制，允許神經網路在處理序列數據時，動態地為輸入的不同部分分配不同的權重。它是 Transformer 架構的核心。

## B

- **Backpropagation (反向傳播)**
  一種用於訓練多層神經網路的演算法。它透過計算輸出誤差，並將誤差信號從輸出層逐層向後傳播，來高效地更新網路的權重。

- **BERT (Bidirectional Encoder Representations from Transformers)**
  由 Google 於 2018 年推出的預訓練語言模型。它使用雙向 Transformer 編碼器，在多項 NLP 任務上取得了破紀錄的成果。

## C

- **CNN (Convolutional Neural Network / 卷積神經網路)**
  一種特別適用於處理網格狀數據（如圖像）的深度學習模型。它透過卷積層和池化層來自動學習圖像的空間層次特徵。

- **Cybernetics (控制論)**
  由諾伯特·維納提出的理論，研究動物和機器中的控制與通訊系統，核心概念是「回饋」。

## D

- **Deepfake (深度偽造)**
  利用深度學習技術（特別是生成模型）創建的虛假圖像或影片，可以將一個人的臉嫁接到另一個人的影片上，達到以假亂真的效果。

- **Diffusion Model (擴散模型)**
  一種強大的生成模型，透過學習從純雜訊中逐步「去噪」來還原數據的過程，從而生成高品質、高解析度的內容（特別是圖像）。

- **Dropout**
  一種在訓練深度學習模型時使用的正則化技術，透過在每次迭代中隨機「丟棄」一部分神經元及其連接，來有效防止模型過擬合。

## E

- **Expert System (專家系統)**
  1980 年代流行的一種 AI 程式，它將特定領域專家的知識編碼成大量的「IF-THEN」規則，用以模擬專家的決策過程。

## G

- **GAN (Generative Adversarial Network / 生成對抗網路)**
  一種由「生成器」和「判別器」兩個網路組成的生成模型。兩者相互對抗訓練，最終生成器能產生非常逼真的數據。

- **GPU (Graphics Processing Unit / 圖形處理器)**
  最初為圖形渲染設計的處理器，其大規模並行計算架構恰好非常適合神經網路的矩陣運算，成為推動深度學習革命的關鍵硬體。

- **GPT (Generative Pre-trained Transformer)**
  由 OpenAI 開發的一系列大型語言模型。它使用 Transformer 的解碼器架構，透過在海量文本上進行預訓練來學習生成連貫的文本。

## I

- **ImageNet**
  一個大規模的標註圖像數據集，包含超過 1400 萬張圖片和 2 萬個類別。其年度競賽 (ILSVRC) 成為推動電腦視覺發展的重要平台。

## L

- **LLM (Large Language Model / 大型語言模型)**
  指參數規模巨大（通常在數十億以上）的語言模型，如 GPT-3、GPT-4 等。它們透過在海量文本數據上預訓練，展現出強大的語言理解和生成能力。

- **LISP (List Processing)**
  由約翰·麥卡錫於 1958 年發明的程式語言，是第一個專為 AI 研究設計的語言，其核心是符號處理和串列結構。

- **LSTM (Long Short-Term Memory / 長短期記憶網路)**
  一種特殊的遞迴神經網路 (RNN)，透過引入「門控」機制來解決標準 RNN 的梯度消失/爆炸問題，能夠更好地學習序列中的長距離依賴關係。

## M

- **Multimodal AI (多模態 AI)**
  能夠同時理解和處理多種不同類型數據（如文本、圖像、音訊、影片）的 AI 系統，例如 GPT-4V。

## P

- **Perceptron (感知器)**
  由 Frank Rosenblatt 於 1958 年發明，是第一個可以從數據中學習的人工神經網路模型，但其能力僅限於解決線性可分問題。

## R

- **ReLU (Rectified Linear Unit / 整流線性單元)**
  一種激活函數，其形式為 `f(x) = max(0, x)`。相比 Sigmoid 等傳統激活函數，它能有效緩解梯度消失問題，大幅加速深度網路的訓練。

- **ResNet (Residual Network / 殘差網路)**
  由何凱明團隊於 2015 年提出，透過引入「殘差連接」（或稱「捷徑連接」），使得訓練數百甚至上千層的超深度神經網路成為可能。

- **RLHF (Reinforcement Learning from Human Feedback / 從人類回饋中強化學習)**
  一種訓練和對齊語言模型的技術。它透過收集人類對模型輸出的偏好排序，來訓練一個獎勵模型，然後用強化學習演算法根據該獎勵模型來微調語言模型，使其輸出更符合人類的期望。

## S

- **Scaling Laws (規模化定律)**
  指 AI 模型（特別是 LLM）的性能與其規模（包括模型參數、數據集大小和計算量）之間存在的可預測的冪律關係。簡而言之，模型越大，性能越好。

- **Symbolic AI (符號主義 AI)**
  AI 的早期研究範式，認為智慧的核心是透過操作符號和應用邏輯規則來進行推理。

## T

- **Transformer**
  由 Google 於 2017 年提出的深度學習架構。它完全基於自注意力機制，拋棄了傳統的遞迴結構，能夠高效地並行處理序列數據，已成為幾乎所有現代大型語言模型的基礎。

- **Turing Test (圖靈測試)**
  由艾倫·圖靈於 1950 年提出的思想實驗，用以判斷機器是否能展現出與人類無法區分的智慧行為。如果一個評審無法透過文字對話穩定地區分出機器和人類，那麼該機器就通過了圖靈測試。

---

[返回目錄](./README_b02.md)
