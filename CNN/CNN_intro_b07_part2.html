<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CNN_intro_b07_part2</title>
    
    <!-- CSS 框架 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/dark.min.css">
    
    <!-- 代碼高亮 -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/base16/darcula.min.css">
    
    <!-- 自定義樣式 -->
    <style>
        
                body {
                    max-width: 900px;
                    margin: 0 auto;
                    padding: 20px;
                }
                /* 提升代码块对比度 */
                pre {
                    background: #1e1e1e !important;
                    border: 1px solid #3e3e3e;
                }
                pre code {
                    background: #1e1e1e !important;
                }
                code {
                    background: #2d2d2d !important;
                }
                /* 引用块对比度 */
                blockquote {
                    background: #2d2d2d;
                    border-left: 4px solid #4a9eff;
                }
            
        
        /* 通用代碼塊樣式 */
        pre code {
            display: block;
            padding: 1.5em;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.6;
        }

        /* Mermaid 圖表樣式 */
        
        .mermaid {
            margin: 2em 0;
            padding: 1.5em;
            text-align: center;
            border-radius: 8px;
        }
        
        .mermaid {
            background: #2c3034;
            border: 1px solid #444;
        }
            
        
        /* 響應式調整 */
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            .container, .markdown-body, .latex-body, .window-body, .nes-container {
                padding: 15px;
            }
            pre code {
                padding: 1em;
            }
        }
        
        
    </style>
</head>
<body>
    <div class="container"><!-- Path: General_python/CNN | Timestamp: 2025-10-07 13:31:00 | Version: b07_part2 -->
<h1>從經典案例學習 CNN：PyTorch 深度實戰 (Part 2)</h1>
<blockquote>
<p>本文件是 CNN 完整實戰指南的第二部分，專注於 <strong>PyTorch 框架</strong>的深度學習實作。</p>
<p><strong>前置閱讀</strong>：建議先閱讀 <code>CNN_intro_b07.md</code> 了解基礎知識與 Keras 實作。</p>
</blockquote>
<hr />
<h2>本文件內容</h2>
<ul>
<li><a href="#第三部分mnist--simplecnn-pytorch">第三部分：MNIST + SimpleCNN (PyTorch)</a></li>
<li><a href="#pytorch-核心概念">PyTorch 核心概念</a></li>
<li><a href="#simplecnn-架構設計">SimpleCNN 架構設計</a></li>
<li><a href="#完整實作程式碼">完整實作程式碼</a></li>
<li><a href="#程式碼深度解析">程式碼深度解析</a></li>
<li><a href="#keras-vs-pytorch-實作對比">Keras vs PyTorch 實作對比</a></li>
</ul>
<hr />
<h2>第三部分：MNIST + SimpleCNN (PyTorch)</h2>
<p>在第二部分中，我們使用 Keras 實作了經典的 LeNet-5。現在讓我們用 <strong>PyTorch</strong> 從頭實作一個現代化的 CNN 模型，深入理解深度學習的訓練流程。</p>
<h3>為什麼要學 PyTorch？</h3>
<p>雖然 Keras 簡潔易用，但 PyTorch 能讓你：</p>
<ol>
<li><strong>深入理解原理</strong>：手寫訓練迴圈，清楚每個步驟</li>
<li><strong>靈活客製化</strong>：更容易實作複雜模型</li>
<li><strong>學術研究</strong>：大多數最新論文使用 PyTorch</li>
<li><strong>動態計算圖</strong>：更直覺、易於除錯</li>
</ol>
<h3>PyTorch 核心概念</h3>
<p>在開始實作前，先理解 PyTorch 的幾個核心概念：</p>
<h4>1. Tensor（張量）</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 創建張量（類似 NumPy array，但可在 GPU 上運算）</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># 在 GPU 上運算</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># 移到 GPU</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>   <span class="c1"># 移回 CPU</span>
</code></pre></div>

<h4>2. nn.Module（模型基類）</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 定義層</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 定義前向傳播</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<h4>3. DataLoader（資料載入器）</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="c1"># 創建 Dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">,</span> <span class="n">y_tensor</span><span class="p">)</span>

<span class="c1"># 創建 DataLoader（自動分批、打亂）</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 迭代批次資料</span>
<span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="c1"># 訓練程式碼</span>
    <span class="k">pass</span>
</code></pre></div>

<h4>4. 訓練迴圈結構</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># PyTorch 訓練的標準流程</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># 1. 清空梯度</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 2. 前向傳播</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>

        <span class="c1"># 3. 計算損失</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span>

        <span class="c1"># 4. 反向傳播</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 5. 更新參數</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3>SimpleCNN 架構設計</h3>
<p>我們將實作一個比 LeNet-5 更現代化的 CNN 架構：</p>
<div class="codehilite"><pre><span></span><code><span class="nf">graph</span><span class="w"> </span><span class="n">LR</span>
<span class="w">    </span><span class="n">Input</span><span class="p">[</span><span class="err">輸入</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="mi">28</span><span class="err">×</span><span class="mi">28</span><span class="err">×</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Conv1</span><span class="p">[</span><span class="n">Conv1</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="mi">32</span><span class="w"> </span><span class="n">filters</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">ReLU</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">28</span><span class="err">×</span><span class="mi">28</span><span class="err">×</span><span class="mi">32</span><span class="p">]</span>
<span class="w">    </span><span class="n">Conv1</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Conv2</span><span class="p">[</span><span class="n">Conv2</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="mi">32</span><span class="w"> </span><span class="n">filters</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">ReLU</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">28</span><span class="err">×</span><span class="mi">28</span><span class="err">×</span><span class="mi">32</span><span class="p">]</span>
<span class="w">    </span><span class="n">Conv2</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Pool1</span><span class="p">[</span><span class="n">MaxPool</span><span class="w"> </span><span class="mi">2</span><span class="err">×</span><span class="mi">2</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">14</span><span class="err">×</span><span class="mi">14</span><span class="err">×</span><span class="mi">32</span><span class="p">]</span>
<span class="w">    </span><span class="n">Pool1</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Drop1</span><span class="p">[</span><span class="n">Dropout</span><span class="w"> </span><span class="mf">0.25</span><span class="p">]</span>

<span class="w">    </span><span class="n">Drop1</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Conv3</span><span class="p">[</span><span class="n">Conv3</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="mi">64</span><span class="w"> </span><span class="n">filters</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">ReLU</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">14</span><span class="err">×</span><span class="mi">14</span><span class="err">×</span><span class="mi">64</span><span class="p">]</span>
<span class="w">    </span><span class="n">Conv3</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Conv4</span><span class="p">[</span><span class="n">Conv4</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="mi">64</span><span class="w"> </span><span class="n">filters</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">ReLU</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">14</span><span class="err">×</span><span class="mi">14</span><span class="err">×</span><span class="mi">64</span><span class="p">]</span>
<span class="w">    </span><span class="n">Conv4</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Pool2</span><span class="p">[</span><span class="n">MaxPool</span><span class="w"> </span><span class="mi">2</span><span class="err">×</span><span class="mi">2</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">7</span><span class="err">×</span><span class="mi">7</span><span class="err">×</span><span class="mi">64</span><span class="p">]</span>
<span class="w">    </span><span class="n">Pool2</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Drop2</span><span class="p">[</span><span class="n">Dropout</span><span class="w"> </span><span class="mf">0.25</span><span class="p">]</span>

<span class="w">    </span><span class="n">Drop2</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Flatten</span><span class="p">[</span><span class="n">Flatten</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="err">→</span><span class="w"> </span><span class="mi">3136</span><span class="p">]</span>
<span class="w">    </span><span class="n">Flatten</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">FC1</span><span class="p">[</span><span class="n">FC</span><span class="w"> </span><span class="mi">128</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">ReLU</span><span class="p">]</span>
<span class="w">    </span><span class="n">FC1</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Drop3</span><span class="p">[</span><span class="n">Dropout</span><span class="w"> </span><span class="mf">0.5</span><span class="p">]</span>
<span class="w">    </span><span class="n">Drop3</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">FC2</span><span class="p">[</span><span class="n">FC</span><span class="w"> </span><span class="mi">10</span><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span><span class="n">Softmax</span><span class="p">]</span>

<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="mi">90</span><span class="n">EE90</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Conv1</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFE4B5</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Conv2</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFE4B5</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Conv3</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFE4B5</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Conv4</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFE4B5</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Pool1</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="mi">87</span><span class="n">CEEB</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Pool2</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="mi">87</span><span class="n">CEEB</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Drop1</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFB6C1</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Drop2</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFB6C1</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">Drop3</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFB6C1</span>
<span class="w">    </span><span class="n">style</span><span class="w"> </span><span class="n">FC2</span><span class="w"> </span><span class="n">fill</span><span class="o">:</span><span class="err">#</span><span class="n">FFD700</span>
</code></pre></div>

<p><strong>架構特色</strong>：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>LeNet-5</th>
<th>SimpleCNN (本實作)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>卷積層數</strong></td>
<td>2</td>
<td>4（兩層一組）</td>
</tr>
<tr>
<td><strong>池化方式</strong></td>
<td>Average Pooling</td>
<td>Max Pooling</td>
</tr>
<tr>
<td><strong>激活函數</strong></td>
<td>tanh</td>
<td>ReLU</td>
</tr>
<tr>
<td><strong>正規化</strong></td>
<td>無</td>
<td>Dropout (0.25, 0.5)</td>
</tr>
<tr>
<td><strong>卷積核大小</strong></td>
<td>5×5</td>
<td>3×3（更現代）</td>
</tr>
<tr>
<td><strong>參數量</strong></td>
<td>~60K</td>
<td>~140K</td>
</tr>
<tr>
<td><strong>預期準確率</strong></td>
<td>98.5%</td>
<td>99.3%+</td>
</tr>
</tbody>
</table>
<h3>完整實作程式碼</h3>
<p>以下程式碼可直接在 Google Colab 執行：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ============================================</span>
<span class="c1"># MNIST + SimpleCNN 完整實作（PyTorch）</span>
<span class="c1"># 執行環境：Google Colab</span>
<span class="c1"># 預期訓練時間：3-5 分鐘（GPU）</span>
<span class="c1"># 預期準確率：&gt;99.3%</span>
<span class="c1"># ============================================</span>

<span class="c1"># ========== 儲存格 1: 導入套件 ==========</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 檢查 PyTorch 版本和 GPU</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch 版本: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA 可用: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU 型號: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 設定裝置</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">使用裝置: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 設定隨機種子</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># ========== 儲存格 2: 載入資料 ==========</span>
<span class="c1"># 使用 torchvision 載入 MNIST</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="c1"># 定義資料轉換（正規化）</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>  <span class="c1"># 轉換為 Tensor，自動正規化到 [0, 1]</span>
    <span class="c1"># MNIST 標準化參數（來自訓練集統計）</span>
    <span class="c1"># 均值 = 0.1307, 標準差 = 0.3081</span>
    <span class="c1"># 這些值是 MNIST 訓練集的全體像素均值與標準差，為公開常用數值</span>
    <span class="c1"># 計算方式：train_images.mean() / 255, train_images.std() / 255</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
<span class="p">])</span>

<span class="c1"># 下載並載入訓練集</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="c1"># 下載並載入測試集</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;訓練集大小: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 60000</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;測試集大小: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>    <span class="c1"># 10000</span>

<span class="c1"># 視覺化前 25 張影像</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="c1"># 注意：transform 已套用，需要反正規化才能正確顯示</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;MNIST 訓練集範例&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># ========== 儲存格 3: 創建 DataLoader ==========</span>
<span class="c1"># 批次大小</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># 創建 DataLoader</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>      <span class="c1"># 訓練集打亂</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>     <span class="c1"># 多執行緒載入（加速）</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>    <span class="c1"># 如果使用 GPU，這會加速資料傳輸</span>
<span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">test_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>     <span class="c1"># 測試集不打亂</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;訓練批次數: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 60000 / 128 ≈ 469</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;測試批次數: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>    <span class="c1"># 10000 / 128 ≈ 79</span>

<span class="c1"># 檢查一個批次的資料</span>
<span class="n">sample_batch_x</span><span class="p">,</span> <span class="n">sample_batch_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">批次資料形狀: </span><span class="si">{</span><span class="n">sample_batch_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (128, 1, 28, 28)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;批次標籤形狀: </span><span class="si">{</span><span class="n">sample_batch_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>    <span class="c1"># (128,)</span>

<span class="c1"># ========== 儲存格 4: 定義模型 ==========</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SimpleCNN 架構</span>
<span class="sd">    - 4 個卷積層（兩層一組）</span>
<span class="sd">    - 2 個最大池化層</span>
<span class="sd">    - 3 個 Dropout 層（防過擬合）</span>
<span class="sd">    - 2 個全連接層</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># ===== 第一組卷積塊 =====</span>
        <span class="c1"># Conv1: 1 → 32 通道，3×3 卷積核，padding=1（保持尺寸）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                               <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Conv2: 32 → 32 通道</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># MaxPool: 28×28 → 14×14</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>

        <span class="c1"># ===== 第二組卷積塊 =====</span>
        <span class="c1"># Conv3: 32 → 64 通道</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Conv4: 64 → 64 通道</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># MaxPool: 14×14 → 7×7</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>

        <span class="c1"># ===== 全連接層 =====</span>
        <span class="c1"># Flatten 後: 7×7×64 = 3136</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        前向傳播</span>
<span class="sd">        輸入: (batch_size, 1, 28, 28)</span>
<span class="sd">        輸出: (batch_size, 10)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 第一組卷積塊</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>      <span class="c1"># (N, 32, 28, 28)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>      <span class="c1"># (N, 32, 28, 28)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>              <span class="c1"># (N, 32, 14, 14)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 第二組卷積塊</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>      <span class="c1"># (N, 64, 14, 14)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>      <span class="c1"># (N, 64, 14, 14)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>              <span class="c1"># (N, 64, 7, 7)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 展平</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># (N, 3136)</span>

        <span class="c1"># 全連接層</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>        <span class="c1"># (N, 128)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># (N, 10)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 創建模型並移至 GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleCNN</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 顯示模型架構</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># 計算參數量</span>
<span class="k">def</span><span class="w"> </span><span class="nf">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="n">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;總參數量: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 測試前向傳播</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;輸入形狀: </span><span class="si">{</span><span class="n">sample_input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;輸出形狀: </span><span class="si">{</span><span class="n">sample_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ========== 儲存格 5: 定義損失函數和優化器 ==========</span>
<span class="c1"># 損失函數：交叉熵（內建 Softmax）</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># 優化器：Adam</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># 學習率調度器：每 5 個 epoch 學習率乘以 0.5</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;損失函數: CrossEntropyLoss&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;優化器: Adam (lr=0.001)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;學習率調度: StepLR (step_size=5, gamma=0.5)&quot;</span><span class="p">)</span>

<span class="c1"># ========== 儲存格 6: 訓練函數 ==========</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    訓練一個 epoch</span>
<span class="sd">    返回: 平均損失, 準確率</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># 設為訓練模式（啟用 Dropout）</span>

    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># 移至 GPU</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 1. 清空梯度</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 2. 前向傳播</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># 3. 計算損失</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># 4. 反向傳播</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 5. 更新參數</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># 統計</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">return</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span>

<span class="c1"># ========== 儲存格 7: 評估函數 ==========</span>
<span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    評估模型</span>
<span class="sd">    返回: 平均損失, 準確率</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 設為評估模式（關閉 Dropout）</span>

    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># 不計算梯度（節省記憶體、加速）</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># 前向傳播</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

            <span class="c1"># 計算損失</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="c1"># 統計</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">return</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span>

<span class="c1"># ========== 儲存格 8: 訓練迴圈 ==========</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># 記錄訓練歷史</span>
<span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s1">&#39;train_acc&#39;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;開始訓練...&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Early Stopping 設定（類似 Keras 的 EarlyStopping callback）</span>
<span class="n">patience</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 容忍 10 個 epochs 沒進步</span>
<span class="n">epochs_no_improve</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 計數器</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">NUM_EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># 訓練</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span>
    <span class="p">)</span>

    <span class="c1"># 評估</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span>
    <span class="p">)</span>

    <span class="c1"># 更新學習率</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">current_lr</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="c1"># 記錄歷史</span>
    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;test_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>

    <span class="c1"># 儲存最佳模型 &amp; Early Stopping 邏輯</span>
    <span class="k">if</span> <span class="n">test_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
        <span class="n">best_acc</span> <span class="o">=</span> <span class="n">test_acc</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;best_simplecnn.pth&#39;</span><span class="p">)</span>
        <span class="n">epochs_no_improve</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 重置計數器</span>
        <span class="n">best_marker</span> <span class="o">=</span> <span class="s1">&#39;⭐&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">NUM_EPOCHS</span><span class="si">}</span><span class="s2">] &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Test Acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;LR: </span><span class="si">{</span><span class="n">current_lr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">best_marker</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">epochs_no_improve</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">best_marker</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">NUM_EPOCHS</span><span class="si">}</span><span class="s2">] &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Test Acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;LR: </span><span class="si">{</span><span class="n">current_lr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (沒進步: </span><span class="si">{</span><span class="n">epochs_no_improve</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">patience</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="c1"># Early Stopping 檢查</span>
    <span class="k">if</span> <span class="n">epochs_no_improve</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">⚠ Early Stopping 觸發！連續 </span><span class="si">{</span><span class="n">patience</span><span class="si">}</span><span class="s2"> 個 epochs 測試準確率未提升&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;最佳測試準確率: </span><span class="si">{</span><span class="n">best_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✓ 訓練完成！最佳測試準確率: </span><span class="si">{</span><span class="n">best_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># ========== 儲存格 9: 視覺化訓練歷史 ==========</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># 準確率曲線</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;訓練準確率&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;測試準確率&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;SimpleCNN 訓練準確率&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;準確率 (%)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># 損失曲線</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;訓練損失&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;test_loss&#39;</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;測試損失&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;SimpleCNN 訓練損失&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;損失值&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># ========== 儲存格 10: 詳細評估 ==========</span>
<span class="c1"># 載入最佳模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;best_simplecnn.pth&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 收集所有預測結果</span>
<span class="n">all_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_targets</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">all_preds</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">all_targets</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">all_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_preds</span><span class="p">)</span>
<span class="n">all_targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_targets</span><span class="p">)</span>

<span class="c1"># 混淆矩陣</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">all_targets</span><span class="p">,</span> <span class="n">all_preds</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SimpleCNN 混淆矩陣&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;預測標籤&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;真實標籤&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 分類報告</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">分類報告:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">all_targets</span><span class="p">,</span> <span class="n">all_preds</span><span class="p">,</span>
                           <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;數字 </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]))</span>

<span class="c1"># ========== 儲存格 11: 預測範例視覺化 ==========</span>
<span class="c1"># 隨機選擇 20 張測試影像</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">),</span> <span class="mi">20</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># 取得影像和標籤</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">true_label</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">img_display</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># 預測</span>
    <span class="n">img_batch</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># 增加 batch 維度</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img_batch</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">confidence</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">pred_label</span> <span class="o">=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">confidence</span> <span class="o">=</span> <span class="n">confidence</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>

    <span class="c1"># 顯示</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_display</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span> <span class="k">if</span> <span class="n">pred_label</span> <span class="o">==</span> <span class="n">true_label</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;真實: </span><span class="si">{</span><span class="n">true_label</span><span class="si">}</span><span class="s1"> | 預測: </span><span class="si">{</span><span class="n">pred_label</span><span class="si">}</span><span class="se">\n</span><span class="s1">信心度: </span><span class="si">{</span><span class="n">confidence</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;SimpleCNN 預測結果（綠色=正確，紅色=錯誤）&#39;</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># ========== 儲存格 12: 每個類別的準確率 ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">每個數字的準確率:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

<span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">all_targets</span> <span class="o">==</span> <span class="n">digit</span>
    <span class="n">digit_acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_preds</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">==</span> <span class="n">all_targets</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">digit_count</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;數字 </span><span class="si">{</span><span class="n">digit</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">digit_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% (</span><span class="si">{</span><span class="n">digit_count</span><span class="si">}</span><span class="s2"> 張)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✓ 所有程式碼執行完成！&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h3>程式碼深度解析</h3>
<h4>1. 為什麼使用 <code>nn.Conv2d(padding=1)</code>？</h4>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p><strong>Padding 的作用</strong>：</p>
<table>
<thead>
<tr>
<th>設定</th>
<th>輸入</th>
<th>輸出</th>
<th>說明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>padding=0</code>（無）</td>
<td>28×28</td>
<td>26×26</td>
<td>邊緣資訊流失</td>
</tr>
<tr>
<td><code>padding=1</code>（推薦）</td>
<td>28×28</td>
<td>28×28</td>
<td>保持尺寸</td>
</tr>
</tbody>
</table>
<p><strong>計算公式</strong>：</p>
<div class="codehilite"><pre><span></span><code>輸出尺寸 = (輸入尺寸 - 卷積核大小 + 2×padding) / stride + 1

範例：
(28 - 3 + 2×1) / 1 + 1 = 28
</code></pre></div>

<p><strong>為什麼保持尺寸？</strong><br />
- 避免過早縮小特徵圖<br />
- 邊緣資訊也能充分利用<br />
- 用池化層控制降維</p>
<h4>2. <code>model.train()</code> vs <code>model.eval()</code></h4>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># 訓練模式</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>   <span class="c1"># 評估模式</span>
</code></pre></div>

<p><strong>差異</strong>：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>Dropout</th>
<th>BatchNorm</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>train()</code></td>
<td>啟用（隨機丟棄）</td>
<td>更新統計量</td>
<td>訓練</td>
</tr>
<tr>
<td><code>eval()</code></td>
<td>關閉（保留所有）</td>
<td>使用固定統計量</td>
<td>評估/預測</td>
</tr>
</tbody>
</table>
<p><strong>重要性</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ❌ 錯誤：評估時忘記設為 eval()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Dropout 會隨機丟棄神經元</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>  <span class="c1"># 準確率會偏低！</span>

<span class="c1"># ✅ 正確</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</code></pre></div>

<h4>3. <code>torch.no_grad()</code> 的作用</h4>
<div class="codehilite"><pre><span></span><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>

<p><strong>作用</strong>：<br />
- 不計算梯度（不需要反向傳播）<br />
- <strong>節省記憶體</strong>：梯度佔用大量空間<br />
- <strong>加速運算</strong>：減少計算量</p>
<p><strong>效能對比</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 測試集評估（10,000 張影像）</span>
<span class="c1"># 有 gradient：約 800MB GPU 記憶體</span>
<span class="c1"># 無 gradient：約 300MB GPU 記憶體（節省 60%）</span>
</code></pre></div>

<h4>4. <code>view()</code> vs <code>reshape()</code> vs <code>flatten()</code></h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 展平操作的三種方法</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># PyTorch 傳統方法</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># NumPy 風格</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>        <span class="c1"># 從第 1 維開始展平</span>
</code></pre></div>

<p><strong>差異</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>特點</th>
<th>建議</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>view()</code></td>
<td>要求記憶體連續</td>
<td>最快，但可能失敗</td>
</tr>
<tr>
<td><code>reshape()</code></td>
<td>自動處理不連續</td>
<td>推薦（穩定）</td>
</tr>
<tr>
<td><code>flatten()</code></td>
<td>語意清晰</td>
<td>CNN 中最推薦</td>
</tr>
</tbody>
</table>
<p><strong>實際使用</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 推薦寫法</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 保留 batch 維度</span>

<span class="c1"># 等價於</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<h4>5. CrossEntropyLoss 的細節</h4>
<div class="codehilite"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<p><strong>重要觀念</strong>：</p>
<p>PyTorch 的 <code>CrossEntropyLoss</code> = <code>LogSoftmax</code> + <code>NLLLoss</code></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 內部實際做的事：</span>
<span class="c1"># 1. 對 output 套用 LogSoftmax</span>
<span class="c1"># 2. 計算負對數似然損失</span>

<span class="c1"># 因此，模型輸出不需要 Softmax！</span>
<span class="c1"># ✅ 正確</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>  <span class="c1"># 直接返回 logits</span>

<span class="c1"># ❌ 錯誤（會導致數值不穩定）</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 不需要！</span>
</code></pre></div>

<p><strong>但在預測時需要 Softmax</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 訓練時</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># 內建 Softmax</span>

<span class="c1"># 預測時（取得機率）</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 手動套用</span>
</code></pre></div>

<h4>6. 優化器的 <code>zero_grad()</code> 為什麼必須？</h4>
<div class="codehilite"><pre><span></span><code><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># 必須！</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<p><strong>原因</strong>：PyTorch 的梯度是<strong>累積</strong>的</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 範例：如果不清空梯度</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># 忘記 zero_grad()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># 輸出（梯度不斷累積）：</span>
<span class="c1"># Epoch 1: 0.015</span>
<span class="c1"># Epoch 2: 0.030  ← 累積了！</span>
<span class="c1"># Epoch 3: 0.045  ← 繼續累積！</span>

<span class="c1"># 正確做法</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># 每次清空</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># 輸出：</span>
<span class="c1"># Epoch 1: 0.015</span>
<span class="c1"># Epoch 2: 0.015  ← 正確</span>
<span class="c1"># Epoch 3: 0.015  ← 正確</span>
</code></pre></div>

<hr />
<h3>Keras vs PyTorch 實作對比</h3>
<p>讓我們直接對比兩個框架的關鍵差異：</p>
<h4>1. 模型定義</h4>
<p><strong>Keras (Sequential API)</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<p><strong>PyTorch (nn.Module)</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">13</span><span class="o">*</span><span class="mi">13</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
</code></pre></div>

<p><strong>差異</strong>：<br />
- Keras: 宣告式，層層堆疊<br />
- PyTorch: 命令式，明確定義流程</p>
<h4>2. 訓練流程</h4>
<p><strong>Keras</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 一次設定所有參數</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">)</span>

<span class="c1"># 一行訓練</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>

<p><strong>PyTorch</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 分別定義</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="c1"># 手寫訓練迴圈</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<p><strong>差異</strong>：<br />
- Keras: 高度封裝，簡潔<br />
- PyTorch: 細粒度控制，靈活</p>
<h4>3. 資料格式</h4>
<table>
<thead>
<tr>
<th>框架</th>
<th>影像格式</th>
<th>批次格式</th>
<th>說明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Keras</strong></td>
<td>(H, W, C)</td>
<td>(N, H, W, C)</td>
<td>Channels Last</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>(C, H, W)</td>
<td>(N, C, H, W)</td>
<td>Channels First</td>
</tr>
</tbody>
</table>
<p><strong>實例</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Keras</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (60000, 28, 28, 1)</span>
<span class="c1">#                       ↑高 ↑寬 ↑通道</span>

<span class="c1"># PyTorch</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (60000, 1, 28, 28)</span>
<span class="c1">#                       ↑通道 ↑高 ↑寬</span>
</code></pre></div>

<h4>4. 完整對比表</h4>
<table>
<thead>
<tr>
<th>特性</th>
<th>Keras</th>
<th>PyTorch</th>
<th>建議</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>模型定義</strong></td>
<td>Sequential / Functional API</td>
<td>nn.Module 類別</td>
<td>Keras 快速原型，PyTorch 客製化</td>
</tr>
<tr>
<td><strong>訓練</strong></td>
<td><code>model.fit()</code></td>
<td>手寫訓練迴圈</td>
<td>Keras 簡單，PyTorch 靈活</td>
</tr>
<tr>
<td><strong>資料載入</strong></td>
<td>NumPy array</td>
<td>DataLoader + Dataset</td>
<td>PyTorch 更強大</td>
</tr>
<tr>
<td><strong>GPU 使用</strong></td>
<td>自動</td>
<td>手動 <code>.to(device)</code></td>
<td>Keras 方便，PyTorch 明確</td>
</tr>
<tr>
<td><strong>動態圖</strong></td>
<td>Eager Execution (預設關閉)</td>
<td>預設開啟</td>
<td>PyTorch 更適合 RNN 等動態模型</td>
</tr>
<tr>
<td><strong>除錯</strong></td>
<td>較困難（靜態圖）</td>
<td>容易（Python 原生）</td>
<td>PyTorch 更易除錯</td>
</tr>
<tr>
<td><strong>部署</strong></td>
<td>TF Serving, TFLite</td>
<td>TorchServe, ONNX</td>
<td>Keras 生態更完善</td>
</tr>
<tr>
<td><strong>社群</strong></td>
<td>業界</td>
<td>學術界</td>
<td>根據需求選擇</td>
</tr>
</tbody>
</table>
<hr />
<h3>預期結果</h3>
<p><strong>訓練過程</strong>（GPU，約 3-5 分鐘）：</p>
<div class="codehilite"><pre><span></span><code>開始訓練...
======================================================================
Epoch [ 1/15] Train Loss: 0.1845 | Train Acc: 94.32% | Test Loss: 0.0512 | Test Acc: 98.32% | LR: 0.001000 ⭐
Epoch [ 2/15] Train Loss: 0.0628 | Train Acc: 98.05% | Test Loss: 0.0369 | Test Acc: 98.84% | LR: 0.001000 ⭐
Epoch [ 3/15] Train Loss: 0.0482 | Train Acc: 98.52% | Test Loss: 0.0297 | Test Acc: 99.08% | LR: 0.001000 ⭐
Epoch [ 4/15] Train Loss: 0.0398 | Train Acc: 98.76% | Test Loss: 0.0275 | Test Acc: 99.17% | LR: 0.001000 ⭐
Epoch [ 5/15] Train Loss: 0.0346 | Train Acc: 98.92% | Test Loss: 0.0254 | Test Acc: 99.23% | LR: 0.001000 ⭐
Epoch [ 6/15] Train Loss: 0.0258 | Train Acc: 99.18% | Test Loss: 0.0239 | Test Acc: 99.28% | LR: 0.000500 ⭐
Epoch [ 7/15] Train Loss: 0.0221 | Train Acc: 99.30% | Test Loss: 0.0228 | Test Acc: 99.32% | LR: 0.000500 ⭐
Epoch [ 8/15] Train Loss: 0.0203 | Train Acc: 99.36% | Test Loss: 0.0223 | Test Acc: 99.35% | LR: 0.000500 ⭐
...
Epoch [15/15] Train Loss: 0.0121 | Train Acc: 99.62% | Test Loss: 0.0210 | Test Acc: 99.40% | LR: 0.000031

✓ 訓練完成！最佳測試準確率: 99.40%
</code></pre></div>

<p><strong>分類報告</strong>：</p>
<div class="codehilite"><pre><span></span><code>              precision    recall  f1-score   support

      數字 0       0.99      1.00      1.00       980
      數字 1       0.99      1.00      0.99      1135
      數字 2       0.99      0.99      0.99      1032
      數字 3       0.99      0.99      0.99      1010
      數字 4       0.99      0.99      0.99       982
      數字 5       0.99      0.99      0.99       892
      數字 6       1.00      0.99      0.99       958
      數字 7       0.99      0.99      0.99      1028
      數字 8       0.99      0.99      0.99       974
      數字 9       0.99      0.99      0.99      1009

    accuracy                           0.99     10000
</code></pre></div>

<p><strong>每個數字的準確率</strong>：</p>
<div class="codehilite"><pre><span></span><code>數字 0: 99.69% (980 張)
數字 1: 99.74% (1135 張)
數字 2: 99.32% (1032 張)
數字 3: 99.21% (1010 張)
數字 4: 99.29% (982 張)
數字 5: 99.22% (892 張)
數字 6: 99.48% (958 張)
數字 7: 99.32% (1028 張)
數字 8: 99.18% (974 張)
數字 9: 99.01% (1009 張)
</code></pre></div>

<hr />
<h3>常見問題排解</h3>
<h4>Q1: RuntimeError: Expected all tensors to be on the same device</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># ❌ 錯誤：資料和模型在不同裝置</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># data 還在 CPU</span>

<span class="c1"># ✅ 正確：統一移至 GPU</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>

<h4>Q2: 訓練準確率 99%，測試準確率 95%（過擬合）</h4>
<p>解決方法：<br />
1. <strong>增加 Dropout</strong>：從 0.25 提高到 0.3-0.4<br />
2. <strong>資料增強</strong>：旋轉、位移、縮放<br />
3. <strong>早停</strong>：監控 <code>val_loss</code>，5 個 epoch 沒改善就停止<br />
4. <strong>L2 正規化</strong>：<br />
<code>python
   optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)</code></p>
<h4>Q3: CUDA out of memory</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 減少 batch_size</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># 從 128 減到 64</span>

<span class="c1"># 或清空快取</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<span class="c1"># 檢查記憶體使用</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;已分配: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;快取: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<h4>Q4: 模型不學習（損失不下降）</h4>
<p>檢查清單：<br />
- ✅ 是否忘記 <code>optimizer.zero_grad()</code><br />
- ✅ 學習率是否過小（試試 0.001）<br />
- ✅ 資料是否正規化<br />
- ✅ 標籤格式是否正確（CrossEntropyLoss 需要類別索引，不是 One-Hot）</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ✅ 正確：標籤為類別索引</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>  <span class="c1"># 形狀: (4,)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># ❌ 錯誤：標籤為 One-Hot</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># 形狀: (4, 10)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># 會報錯或結果錯誤</span>
</code></pre></div>

<hr />
<h2>總結與對比</h2>
<h3>LeNet-5 (Keras) vs SimpleCNN (PyTorch)</h3>
<table>
<thead>
<tr>
<th>指標</th>
<th>LeNet-5 (Keras)</th>
<th>SimpleCNN (PyTorch)</th>
<th>差異</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>準確率</strong></td>
<td>98.76%</td>
<td>99.40%</td>
<td>+0.64%</td>
</tr>
<tr>
<td><strong>訓練時間</strong></td>
<td>2-3 分鐘</td>
<td>3-5 分鐘</td>
<td>稍慢（手寫迴圈）</td>
</tr>
<tr>
<td><strong>參數量</strong></td>
<td>~60K</td>
<td>~140K</td>
<td>2.3 倍</td>
</tr>
<tr>
<td><strong>程式碼行數</strong></td>
<td>~150 行</td>
<td>~300 行</td>
<td>PyTorch 更冗長</td>
</tr>
<tr>
<td><strong>學習難度</strong></td>
<td>★☆☆☆☆</td>
<td>★★★☆☆</td>
<td>PyTorch 需理解更多細節</td>
</tr>
<tr>
<td><strong>靈活度</strong></td>
<td>★★☆☆☆</td>
<td>★★★★★</td>
<td>PyTorch 易於客製化</td>
</tr>
</tbody>
</table>
<h3>何時使用哪個框架？</h3>
<p><strong>使用 Keras</strong>：<br />
- ✅ 快速驗證想法<br />
- ✅ 標準的影像分類任務<br />
- ✅ 團隊成員技術水平不一<br />
- ✅ 需要快速部署到生產環境</p>
<p><strong>使用 PyTorch</strong>：<br />
- ✅ 需要深入理解模型細節<br />
- ✅ 研究導向專案<br />
- ✅ 複雜的自訂架構（如 Transformer）<br />
- ✅ 需要細粒度控制訓練過程</p>
<h3>下一步</h3>
<p>現在你已經掌握了：<br />
- ✅ Keras 的高階 API（LeNet-5）<br />
- ✅ PyTorch 的底層控制（SimpleCNN）</p>
<p>準備好挑戰更難的任務了嗎？</p>
<p><strong>請繼續閱讀</strong>：<code>CNN_intro_b07_part3.md</code> - CIFAR-10 彩色影像分類</p>
<hr />
<p><strong>本文件完成時間</strong>：2025-10-07 14:30:00<br />
<strong>版本</strong>：b07_part2<br />
<strong>下一部分</strong>：<code>CNN_intro_b07_part3.md</code> (CIFAR-10 進階實戰)</p></div>
    
    <!-- Highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <!-- Mermaid -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({
            "startOnLoad": true,
            "theme": "dark",
            "securityLevel": "loose"
});
    </script>
</body>
</html>