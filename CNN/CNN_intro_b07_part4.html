<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CNN_intro_b07_part4</title>
    
    <!-- CSS 框架 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/dark.min.css">
    
    <!-- 代碼高亮 -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/base16/darcula.min.css">
    
    <!-- 自定義樣式 -->
    <style>
        
                body {
                    max-width: 900px;
                    margin: 0 auto;
                    padding: 20px;
                }
                /* 提升代碼塊對比度 */
                pre {
                    background: #1e1e1e !important;
                    border: 1px solid #3e3e3e;
                }
                pre code {
                    background: #1e1e1e !important;
                }
                code {
                    background: #2d2d2d !important;
                }
                /* 引用塊對比度 */
                blockquote {
                    background: #2d2d2d;
                    border-left: 4px solid #4a9eff;
                }
                    
        /* 通用代碼塊樣式 */
        pre code {
            display: block;
            padding: 1.5em;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.6;
        }

        /* Mermaid 圖表樣式 */
        
        .mermaid {
            margin: 2em 0;
            padding: 1.5em;
            text-align: center;
            border-radius: 8px;
        }
        
        .mermaid {
            background: #2c3034;
            border: 1px solid #444;
        }
            
        
        /* 響應式調整 */
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            .container, .markdown-body, .latex-body, .window-body, .nes-container { # Added .nes-container here
                padding: 15px;
            }
            pre code {
                padding: 1em;
            }
        }
        
        
    </style>
</head>
<body>
    <div class="container"><!-- Path: General_python/CNN | Timestamp: 2025-10-07 13:36:00 | Version: b07_part4 -->
<h1>從經典案例學習 CNN：實戰技巧與完整總結 (Part 4)</h1>
<blockquote>
<p>本文件是 CNN 完整實戰指南的第四部分（最終篇），總結實戰經驗與進階技巧。</p>
<p><strong>前置閱讀</strong>：建議先完成 Part 1-3。</p>
</blockquote>
<hr />
<h2>本文件內容</h2>
<ul>
<li><a href="#第五部分實戰技巧與疑難排解">第五部分：實戰技巧與疑難排解</a></li>
<li><a href="#常見問題與解決方案">常見問題與解決方案</a></li>
<li><a href="#模型不收斂的診斷流程">模型不收斂的診斷流程</a></li>
<li><a href="#過擬合與欠擬合">過擬合與欠擬合</a></li>
<li><a href="#超參數調整指南">超參數調整指南</a></li>
<li><a href="#效能優化技巧">效能優化技巧</a></li>
<li><a href="#進階主題導覽">進階主題導覽</a></li>
<li><a href="#完整學習路線圖">完整學習路線圖</a></li>
<li><a href="#總結與下一步">總結與下一步</a></li>
</ul>
<hr />
<h2>第五部分：實戰技巧與疑難排解</h2>
<h3>常見問題與解決方案</h3>
<h4>問題 1：準確率卡在某個值不上升</h4>
<p><strong>症狀</strong>：</p>
<div class="codehilite"><pre><span></span><code>Epoch 1: Train Acc 65%, Test Acc 63%
Epoch 5: Train Acc 75%, Test Acc 72%
Epoch 10: Train Acc 76%, Test Acc 72%  ← 停滯
Epoch 20: Train Acc 76%, Test Acc 72%
</code></pre></div>

<p><strong>可能原因與解決方案</strong>：</p>
<div class="mermaid">
graph TD
    Problem[準確率停滯] --&gt; Check1{學習率}
    Problem --&gt; Check2{模型容量}
    Problem --&gt; Check3{資料品質}
    Problem --&gt; Check4{損失函數}

    Check1 --&gt;|過大| Sol1[降低學習率&lt;br/&gt;0.001 → 0.0001]
    Check1 --&gt;|過小| Sol2[提高學習率&lt;br/&gt;使用學習率調度]

    Check2 --&gt;|太小| Sol3[增加層數/濾波器&lt;br/&gt;64→128→256]
    Check2 --&gt;|太大| Sol4[減少參數&lt;br/&gt;加強正規化]

    Check3 --&gt;|標籤錯誤| Sol5[檢查資料標註&lt;br/&gt;清理異常樣本]
    Check3 --&gt;|不平衡| Sol6[使用加權損失&lt;br/&gt;過採樣/欠採樣]

    Check4 --&gt;|不適合| Sol7[換損失函數&lt;br/&gt;Focal Loss]

    style Problem fill:#FFB6C1
    style Sol1 fill:#90EE90
    style Sol2 fill:#90EE90
    style Sol3 fill:#90EE90
    style Sol4 fill:#90EE90
    style Sol5 fill:#90EE90
    style Sol6 fill:#90EE90
    style Sol7 fill:#90EE90
</div>
<p><strong>診斷步驟</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 步驟 1: 檢查學習率 =====</span>
<span class="c1"># 畫出損失曲線，觀察震盪情況</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;訓練損失曲線&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 如果損失劇烈震盪 → 學習率過大</span>
<span class="c1"># 如果損失幾乎不動 → 學習率過小</span>

<span class="c1"># ===== 步驟 2: 嘗試更大的學習率範圍 =====</span>
<span class="c1"># Learning Rate Finder</span>
<span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="c1"># 訓練 2-3 個 epoch</span>
    <span class="c1"># 記錄最終損失</span>

<span class="c1"># ===== 步驟 3: 檢查模型容量 =====</span>
<span class="c1"># 計算訓練集和測試集的差距</span>
<span class="n">train_acc</span> <span class="o">=</span> <span class="mi">76</span><span class="o">%</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="mi">72</span><span class="o">%</span>
<span class="n">gap</span> <span class="o">=</span> <span class="mi">4</span><span class="o">%</span>

<span class="k">if</span> <span class="n">gap</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="o">%</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;模型容量可能不足，試著加深/加寬網路&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;模型容量充足，問題可能在學習率或資料&quot;</span><span class="p">)</span>

<span class="c1"># ===== 步驟 4: 視覺化預測 =====</span>
<span class="c1"># 看看錯誤的案例，是否有規律？</span>
<span class="n">wrong_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">!=</span> <span class="n">y_true</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">wrong_indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True: </span><span class="si">{</span><span class="n">y_true</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">, Pred: </span><span class="si">{</span><span class="n">y_pred</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<hr />
<h4>問題 2：訓練集準確率高，測試集低（過擬合）</h4>
<p><strong>症狀</strong>：</p>
<div class="codehilite"><pre><span></span><code>Epoch 20: Train Acc 95%, Test Acc 75%  ← 差距 20%
</code></pre></div>

<p><strong>解決方案優先級</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>效果</th>
<th>實作難度</th>
<th>推薦指數</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>資料增強</strong></td>
<td>+++++</td>
<td>★☆☆☆☆</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>Dropout</strong></td>
<td>++++</td>
<td>★☆☆☆☆</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>L2 正規化</strong></td>
<td>+++</td>
<td>★☆☆☆☆</td>
<td>⭐⭐⭐⭐☆</td>
</tr>
<tr>
<td><strong>早停 (Early Stopping)</strong></td>
<td>++++</td>
<td>★☆☆☆☆</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>Batch Normalization</strong></td>
<td>+++</td>
<td>★★☆☆☆</td>
<td>⭐⭐⭐⭐☆</td>
</tr>
<tr>
<td><strong>減少模型容量</strong></td>
<td>+++</td>
<td>★★☆☆☆</td>
<td>⭐⭐⭐☆☆</td>
</tr>
<tr>
<td><strong>收集更多資料</strong></td>
<td>+++++</td>
<td>★★★★★</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
</tbody>
</table>
<p><strong>完整解決方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 方案 1: 資料增強（最有效） =====</span>
<span class="n">datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
    <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.15</span>
<span class="p">)</span>

<span class="c1"># ===== 方案 2: 加強 Dropout =====</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
    <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>  <span class="c1"># 從 0.25 提高到 0.3-0.4</span>
    <span class="c1"># ...</span>
    <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
    <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># 全連接層用更高的 Dropout</span>
    <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># ===== 方案 3: L2 正規化 =====</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.regularizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">l2</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)))</span>  <span class="c1"># L2 正規化</span>

<span class="c1"># PyTorch: 在優化器中設定 weight_decay</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># ===== 方案 4: 早停 =====</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>         <span class="c1"># 10 個 epoch 沒改善就停</span>
    <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># ===== 方案 5: 減少模型大小 =====</span>
<span class="c1"># 將濾波器數量減半</span>
<span class="c1"># 原本: 64 → 128 → 256 → 512</span>
<span class="c1"># 改為: 32 → 64 → 128 → 256</span>
</code></pre></div>

<hr />
<h4>問題 3：訓練速度太慢</h4>
<p><strong>速度優化檢查清單</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 1. 確認 GPU 已啟用 =====</span>
<span class="c1"># TensorFlow</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">))</span>

<span class="c1"># PyTorch</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>

<span class="c1"># 如果顯示 GPU 但沒加速，檢查資料是否在 GPU 上：</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># PyTorch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># ===== 2. 增加 Batch Size =====</span>
<span class="c1"># GPU 記憶體允許的情況下，越大越快</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># 從 128 提高到 256</span>
<span class="c1"># 注意：Batch Size 提高可能需要調整學習率</span>

<span class="c1"># ===== 3. 使用 Mixed Precision 訓練 =====</span>
<span class="c1"># TensorFlow</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.mixed_precision</span><span class="w"> </span><span class="kn">import</span> <span class="n">Policy</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">(</span><span class="s1">&#39;mixed_float16&#39;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>

<span class="c1"># PyTorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>  <span class="c1"># 自動混合精度</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

<span class="c1"># 效果：速度提升 2-3 倍，記憶體減少 50%</span>

<span class="c1"># ===== 4. 優化 DataLoader =====</span>
<span class="c1"># 增加 num_workers</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>      <span class="c1"># 從 2 提高到 4</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>    <span class="c1"># 加速 CPU→GPU 傳輸</span>
    <span class="n">persistent_workers</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># PyTorch 1.7+</span>
<span class="p">)</span>

<span class="c1"># ===== 5. 使用更快的優化器 =====</span>
<span class="c1"># AdamW 通常比 Adam 更快收斂</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span>  <span class="c1"># Keras</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>  <span class="c1"># PyTorch</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div>

<p><strong>速度對比</strong>（CIFAR-10, 50 epochs）：</p>
<table>
<thead>
<tr>
<th>設定</th>
<th>訓練時間</th>
<th>加速比</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU, batch=128</td>
<td>120 分鐘</td>
<td>1×</td>
</tr>
<tr>
<td>GPU, batch=128</td>
<td>15 分鐘</td>
<td>8×</td>
</tr>
<tr>
<td>GPU, batch=256</td>
<td>10 分鐘</td>
<td>12×</td>
</tr>
<tr>
<td>GPU, batch=256, Mixed Precision</td>
<td>5 分鐘</td>
<td>24×</td>
</tr>
<tr>
<td>GPU, batch=256, MP, num_workers=4</td>
<td>4 分鐘</td>
<td>30×</td>
</tr>
</tbody>
</table>
<hr />
<h4>問題 4：CUDA Out of Memory</h4>
<p><strong>錯誤訊息</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">memory</span><span class="o">.</span><span class="w"> </span><span class="n">Tried</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">allocate</span><span class="w"> </span><span class="mf">1.50</span><span class="w"> </span><span class="n">GiB</span>
<span class="o">(</span><span class="n">GPU</span><span class="w"> </span><span class="mi">0</span><span class="o">;</span><span class="w"> </span><span class="mf">15.75</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">capacity</span><span class="o">;</span><span class="w"> </span><span class="mf">14.23</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">allocated</span><span class="o">)</span>
</code></pre></div>

<p><strong>解決方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 方案 1: 減少 Batch Size（最有效） =====</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># 從 128 減到 64 或 32</span>

<span class="c1"># ===== 方案 2: 清空 GPU 快取 =====</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<span class="c1"># TensorFlow</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>

<span class="c1"># ===== 方案 3: 使用梯度累積（保持有效 Batch Size） =====</span>
<span class="c1"># 模擬 batch_size=128 的效果</span>
<span class="n">ACCUMULATION_STEPS</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">effective_batch_size</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># 32 × 4 = 128</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">/</span> <span class="n">ACCUMULATION_STEPS</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">ACCUMULATION_STEPS</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># ===== 方案 4: 減少模型大小 =====</span>
<span class="c1"># 將濾波器數量減半</span>
<span class="c1"># 或使用 DepthwiseSeparable Convolution</span>

<span class="c1"># ===== 方案 5: 使用 Gradient Checkpointing =====</span>
<span class="c1"># PyTorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.checkpoint</span><span class="w"> </span><span class="kn">import</span> <span class="n">checkpoint</span>

<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 重新計算而非儲存</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<hr />
<h3>模型不收斂的診斷流程</h3>
<div class="mermaid">
graph TD
    Start[模型不收斂] --&gt; Q1{損失是否下降?}

    Q1 --&gt;|完全不降| Path1[檢查基礎設定]
    Q1 --&gt;|下降但停滯| Path2[優化超參數]
    Q1 --&gt;|劇烈震盪| Path3[學習率過大]

    Path1 --&gt; Check1[1. 資料是否正規化?]
    Check1 --&gt; Check2[2. 標籤格式正確?]
    Check2 --&gt; Check3[3. 優化器設定正確?]
    Check3 --&gt; Check4[4. 損失函數適合?]

    Path2 --&gt; Opt1[1. 提高學習率]
    Opt1 --&gt; Opt2[2. 增加模型容量]
    Opt2 --&gt; Opt3[3. 檢查資料品質]

    Path3 --&gt; Fix1[降低學習率&lt;br/&gt;0.001 → 0.0001]

    Check1 --&gt; Fix2[正規化到 [0,1]&lt;br/&gt;或標準化]
    Check2 --&gt; Fix3[Keras: One-Hot&lt;br/&gt;PyTorch: 類別索引]
    Check3 --&gt; Fix4[檢查 zero_grad&lt;br/&gt;檢查 optimizer.step]
    Check4 --&gt; Fix5[分類: CrossEntropy&lt;br/&gt;回歸: MSE]

    style Start fill:#FFB6C1
    style Fix2 fill:#90EE90
    style Fix3 fill:#90EE90
    style Fix4 fill:#90EE90
    style Fix5 fill:#90EE90
    style Fix1 fill:#90EE90
</div>
<p><strong>診斷代碼</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 診斷清單 =====</span>
<span class="k">def</span><span class="w"> </span><span class="nf">diagnose_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;診斷模型訓練問題&quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;模型診斷報告&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

    <span class="c1"># 1. 檢查資料範圍</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. 資料範圍檢查:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   X_train: [</span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ✓ 正常範圍: [0, 1] 或 [-1, 1]&quot;</span> <span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mf">1.5</span> <span class="k">else</span> <span class="s2">&quot;   ✗ 可能需要正規化！&quot;</span><span class="p">)</span>

    <span class="c1"># 2. 檢查標籤格式</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. 標籤格式檢查:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   y_train shape: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   y_train unique: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># 3. 檢查類別平衡</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. 類別平衡檢查:&quot;</span><span class="p">)</span>
    <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">max_count</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">min_count</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">imbalance_ratio</span> <span class="o">=</span> <span class="n">max_count</span> <span class="o">/</span> <span class="n">min_count</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   最多: </span><span class="si">{</span><span class="n">max_count</span><span class="si">}</span><span class="s2">, 最少: </span><span class="si">{</span><span class="n">min_count</span><span class="si">}</span><span class="s2">, 比例: </span><span class="si">{</span><span class="n">imbalance_ratio</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">imbalance_ratio</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ⚠️ 類別不平衡！考慮使用加權損失&quot;</span><span class="p">)</span>

    <span class="c1"># 4. 過擬合檢測（需要訓練後）</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. 過擬合檢測:&quot;</span><span class="p">)</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">gap</span> <span class="o">=</span> <span class="n">train_acc</span> <span class="o">-</span> <span class="n">test_acc</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Test Acc:  </span><span class="si">{</span><span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Gap: </span><span class="si">{</span><span class="n">gap</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gap</span> <span class="o">&gt;</span> <span class="mf">0.15</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ⚠️ 過擬合！建議:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      - 資料增強&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      - 增加 Dropout&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      - L2 正規化&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">gap</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="ow">and</span> <span class="n">test_acc</span> <span class="o">&lt;</span> <span class="mf">0.85</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ⚠️ 模型容量不足！建議:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      - 增加層數&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      - 增加濾波器數量&quot;</span><span class="p">)</span>

    <span class="c1"># 5. 檢查梯度</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. 梯度檢查:&quot;</span><span class="p">)</span>
    <span class="c1"># 訓練一個 batch</span>
    <span class="n">sample_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">32</span><span class="p">]</span>
    <span class="n">sample_labels</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">32</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">categorical_crossentropy</span><span class="p">(</span><span class="n">sample_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

    <span class="c1"># 檢查是否有 NaN 或過大/過小的梯度</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gradients</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">grad_mean</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ✗ 層 </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: 梯度為 NaN！&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">grad_mean</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ⚠️ 層 </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: 梯度過小 (</span><span class="si">{</span><span class="n">grad_mean</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">) - 可能梯度消失&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">grad_mean</span> <span class="o">&gt;</span> <span class="mf">1e2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ⚠️ 層 </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: 梯度過大 (</span><span class="si">{</span><span class="n">grad_mean</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">) - 可能梯度爆炸&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="c1"># 使用範例</span>
<span class="n">diagnose_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_cat</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test_cat</span><span class="p">)</span>
</code></pre></div>

<hr />
<h3>過擬合與欠擬合</h3>
<h4>完整對比</h4>
<div class="mermaid">
graph LR
    subgraph 欠擬合
        U1[訓練準確率 低&lt;br/&gt;測試準確率 低]
        U2[模型太簡單]
        U3[學習不足]
    end

    subgraph 良好擬合
        G1[訓練準確率 高&lt;br/&gt;測試準確率 高]
        G2[泛化能力強]
    end

    subgraph 過擬合
        O1[訓練準確率 高&lt;br/&gt;測試準確率 低]
        O2[記住訓練資料]
        O3[泛化能力弱]
    end

    U1 --&gt; Solution1[增加模型容量&lt;br/&gt;訓練更久&lt;br/&gt;更好的特徵]
    G1 --&gt; Maintain[保持現狀&lt;br/&gt;或繼續優化]
    O1 --&gt; Solution2[資料增強&lt;br/&gt;Dropout&lt;br/&gt;正規化&lt;br/&gt;早停]

    style U1 fill:#FFE4B5
    style G1 fill:#90EE90
    style O1 fill:#FFB6C1
</div>
<h4>視覺化診斷</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 視覺化過擬合/欠擬合</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_learning_curves</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;視覺化學習曲線並診斷問題&quot;&quot;&quot;</span>

    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># 準確率</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;訓練&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;驗證&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;準確率曲線&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;準確率&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># 損失</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;訓練&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;驗證&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;損失曲線&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;損失&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># 診斷</span>
    <span class="n">final_train_acc</span> <span class="o">=</span> <span class="n">train_acc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">final_val_acc</span> <span class="o">=</span> <span class="n">val_acc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">gap</span> <span class="o">=</span> <span class="n">final_train_acc</span> <span class="o">-</span> <span class="n">final_val_acc</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">診斷結果:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_val_acc</span> <span class="o">&lt;</span> <span class="mf">0.7</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gap</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ 欠擬合 (Underfitting)&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   訓練和驗證準確率都很低，但差距小&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">建議:&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   1. 增加模型容量（更多層/濾波器）&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   2. 訓練更多 epochs&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   3. 降低正規化強度&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   4. 檢查學習率是否過小&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;⚠️ 欠擬合 + 輕微過擬合&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   驗證準確率低，但訓練準確率更高&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">建議:&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   1. 先解決欠擬合：增加模型容量&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   2. 再處理過擬合：加資料增強&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">gap</span> <span class="o">&gt;</span> <span class="mf">0.15</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ 過擬合 (Overfitting)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   訓練準確率 </span><span class="si">{</span><span class="n">final_train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%，驗證準確率 </span><span class="si">{</span><span class="n">final_val_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   差距 </span><span class="si">{</span><span class="n">gap</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">建議:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   1. 資料增強（最有效）&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   2. 增加 Dropout (0.3-0.5)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   3. L2 正規化 (weight_decay=1e-4)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   4. 早停 (EarlyStopping)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   5. 減少模型容量&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✅ 良好擬合 (Good Fit)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   訓練準確率 </span><span class="si">{</span><span class="n">final_train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%，驗證準確率 </span><span class="si">{</span><span class="n">final_val_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   差距 </span><span class="si">{</span><span class="n">gap</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%（健康範圍）&quot;</span><span class="p">)</span>

    <span class="c1"># 檢查驗證損失是否上升</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">min_val_loss_epoch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_val_loss_epoch</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span> <span class="o">-</span> <span class="mi">5</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">⚠️ 驗證損失在 epoch </span><span class="si">{</span><span class="n">min_val_loss_epoch</span><span class="si">}</span><span class="s2"> 達到最低&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   之後開始上升，建議在該點停止訓練&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 使用範例</span>
<span class="n">plot_learning_curves</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
</code></pre></div>

<hr />
<h3>超參數調整指南</h3>
<h4>超參數重要性排序</h4>
<div class="mermaid">
graph LR
    P1[學習率&lt;br/&gt;⭐⭐⭐⭐⭐] --&gt; Impact1[影響最大&lt;br/&gt;優先調整]
    P2[Batch Size&lt;br/&gt;⭐⭐⭐⭐☆] --&gt; Impact2[影響訓練速度&lt;br/&gt;與穩定性]
    P3[優化器&lt;br/&gt;⭐⭐⭐☆☆] --&gt; Impact3[Adam 通常足夠]
    P4[網路架構&lt;br/&gt;⭐⭐⭐⭐☆] --&gt; Impact4[層數、濾波器數]
    P5[Dropout 率&lt;br/&gt;⭐⭐⭐☆☆] --&gt; Impact5[防過擬合]
    P6[L2 正規化&lt;br/&gt;⭐⭐☆☆☆] --&gt; Impact6[輔助防過擬合]

    style P1 fill:#FFD700
    style P2 fill:#FFD700
    style P4 fill:#FFD700
    style P3 fill:#FFE4B5
    style P5 fill:#FFE4B5
    style P6 fill:#E0E0E0
</div>
<h4>學習率調整策略</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># ===== 策略 1: 學習率範圍測試 (Learning Rate Finder) =====</span>
<span class="k">def</span><span class="w"> </span><span class="nf">find_learning_rate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;找出最佳學習率範圍&quot;&quot;&quot;</span>
    <span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># 測試從 1e-6 到 1 的學習率</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>  <span class="c1"># 只測試 100 個 batch</span>
            <span class="k">break</span>

        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 更新學習率</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># 記錄</span>
        <span class="n">learning_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># 指數增長學習率</span>
        <span class="n">lr</span> <span class="o">*=</span> <span class="mf">1.1</span>

    <span class="c1"># 視覺化</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Rate Finder&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># 建議：選擇損失下降最快的點</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;建議學習率:&quot;</span><span class="p">)</span>
    <span class="n">min_loss_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  最低損失點: </span><span class="si">{</span><span class="n">learning_rates</span><span class="p">[</span><span class="n">min_loss_idx</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  建議值（除以10）: </span><span class="si">{</span><span class="n">learning_rates</span><span class="p">[</span><span class="n">min_loss_idx</span><span class="p">]</span><span class="o">/</span><span class="mi">10</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ===== 策略 2: 學習率調度 =====</span>
<span class="c1"># 方案 A: Step Decay（階梯式衰減）</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 每 30 個 epoch，學習率乘以 0.1</span>

<span class="c1"># 方案 B: Exponential Decay（指數衰減）</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="c1"># 每個 epoch，學習率乘以 0.95</span>

<span class="c1"># 方案 C: Cosine Annealing（餘弦退火）</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="c1"># 餘弦曲線，從 lr_max 降到 eta_min</span>

<span class="c1"># 方案 D: ReduceLROnPlateau（自適應）</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="c1"># 驗證損失 5 個 epoch 沒改善，學習率減半</span>

<span class="c1"># ===== 策略 3: Warm-up + Cosine Annealing =====</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lr_lambda</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Warm-up</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Cosine Annealing</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">100</span> <span class="o">-</span> <span class="mi">10</span><span class="p">)</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">)</span>
</code></pre></div>

<p><strong>學習率調度對比</strong>：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>曲線</th>
<th>適用場景</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Fixed</strong></td>
<td>平坦</td>
<td>快速實驗</td>
<td>★★☆☆☆</td>
</tr>
<tr>
<td><strong>Step Decay</strong></td>
<td>階梯</td>
<td>傳統訓練</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td><strong>Exponential</strong></td>
<td>指數下降</td>
<td>長時間訓練</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td><strong>Cosine Annealing</strong></td>
<td>餘弦曲線</td>
<td>現代推薦</td>
<td>★★★★☆</td>
</tr>
<tr>
<td><strong>ReduceLROnPlateau</strong></td>
<td>自適應</td>
<td>不確定時</td>
<td>★★★★★</td>
</tr>
<tr>
<td><strong>Warm-up + Cosine</strong></td>
<td>先升後降</td>
<td>大型模型</td>
<td>★★★★★</td>
</tr>
</tbody>
</table>
<h4>Batch Size 的影響</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Batch Size 對比實驗</span>
<span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="c1"># 訓練 10 epochs</span>
    <span class="c1"># ...</span>

    <span class="n">results</span><span class="p">[</span><span class="n">bs</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;train_time&#39;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span>
        <span class="s1">&#39;final_acc&#39;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span>
        <span class="s1">&#39;memory_usage&#39;</span><span class="p">:</span> <span class="o">...</span>
    <span class="p">}</span>

<span class="c1"># 結果範例（CIFAR-10）</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Batch Size | 訓練時間 | 準確率 | GPU 記憶體</span>
<span class="sd">-----------|---------|-------|----------</span>
<span class="sd">32         | 25 min  | 86.2% | 2.5 GB</span>
<span class="sd">64         | 15 min  | 86.5% | 3.8 GB</span>
<span class="sd">128        | 10 min  | 86.8% | 6.2 GB  ← 推薦</span>
<span class="sd">256        | 8 min   | 86.3% | 10.5 GB</span>
<span class="sd">512        | 7 min   | 85.1% | OOM     ← 過大導致泛化變差</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<p><strong>Batch Size 選擇建議</strong>：</p>
<div class="mermaid">
graph TD
    Start{GPU 記憶體}

    Start --&gt;|16GB+| Large[Batch Size 256-512&lt;br/&gt;需調高學習率]
    Start --&gt;|8-16GB| Medium[Batch Size 128-256&lt;br/&gt;最佳選擇]
    Start --&gt;|4-8GB| Small[Batch Size 64-128&lt;br/&gt;平衡效能]
    Start --&gt;|&lt;4GB| Tiny[Batch Size 32-64&lt;br/&gt;或用梯度累積]

    Large --&gt; Note1[學習率 × 2-4]
    Medium --&gt; Note2[學習率保持]
    Small --&gt; Note2
    Tiny --&gt; Note3[考慮梯度累積]

    style Medium fill:#90EE90
    style Small fill:#FFE4B5
    style Large fill:#FFE4B5
    style Tiny fill:#FFB6C1
</div>
<hr />
<h3>效能優化技巧</h3>
<h4>完整優化清單</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># ============================================</span>
<span class="c1"># 完整優化範例（PyTorch）</span>
<span class="c1"># 包含所有效能提升技巧</span>
<span class="c1"># ============================================</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># ===== 1. 使用 Mixed Precision 訓練 =====</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="c1"># ===== 2. 優化 DataLoader =====</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>           <span class="c1"># 較大 batch size</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># 多執行緒</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>          <span class="c1"># 加速 CPU→GPU</span>
    <span class="n">persistent_workers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 保持 workers 活著</span>
    <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">2</span>         <span class="c1"># 預載入</span>
<span class="p">)</span>

<span class="c1"># ===== 3. 優化模型 =====</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 使用 torch.compile（PyTorch 2.0+）</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># 自動優化計算圖</span>

<span class="c1"># ===== 4. 優化優化器 =====</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">foreach</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># 向量化操作（PyTorch 1.12+）</span>
<span class="p">)</span>

<span class="c1"># ===== 5. 訓練迴圈（完整優化版） =====</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Mixed Precision</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># 反向傳播（Mixed Precision）</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 更快的梯度清空</span>

<span class="c1"># ===== 效能對比 =====</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">優化前: 20 分鐘/epoch</span>
<span class="sd">優化後: 5 分鐘/epoch  ← 快 4 倍！</span>

<span class="sd">優化項目           | 提升</span>
<span class="sd">-------------------|------</span>
<span class="sd">Mixed Precision    | 2.5×</span>
<span class="sd">DataLoader 優化    | 1.3×</span>
<span class="sd">torch.compile      | 1.2×</span>
<span class="sd">綜合效果           | 4×</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2>進階主題導覽</h2>
<p>當你完成本指南後，可以探索這些進階主題：</p>
<h3>1. 遷移學習 (Transfer Learning)</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用預訓練模型（以 ResNet 為例）</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span>

<span class="c1"># 載入預訓練模型</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 凍結預訓練層</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">resnet</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># 替換最後一層</span>
<span class="n">resnet</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># CIFAR-10 有 10 類</span>

<span class="c1"># 只訓練最後一層</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">resnet</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># 效果：</span>
<span class="c1"># - 訓練時間: 從 20 分鐘減少到 5 分鐘</span>
<span class="c1"># - 準確率: 從 87% 提升到 94%</span>
</code></pre></div>

<p><strong>進階閱讀</strong>：<br />
- ImageNet 預訓練模型<br />
- Fine-tuning 策略<br />
- Domain Adaptation</p>
<h3>2. 進階 CNN 架構</h3>
<div class="mermaid">
graph LR
    Basic[基礎 CNN] --&gt; Adv1[ResNet&lt;br/&gt;殘差連接]
    Basic --&gt; Adv2[DenseNet&lt;br/&gt;密集連接]
    Basic --&gt; Adv3[Inception&lt;br/&gt;多尺度特徵]
    Basic --&gt; Adv4[EfficientNet&lt;br/&gt;複合縮放]
    Basic --&gt; Adv5[Vision Transformer&lt;br/&gt;注意力機制]

    Adv1 --&gt; Papers1[He et al., 2015]
    Adv2 --&gt; Papers2[Huang et al., 2017]
    Adv3 --&gt; Papers3[Szegedy et al., 2015]
    Adv4 --&gt; Papers4[Tan &amp; Le, 2019]
    Adv5 --&gt; Papers5[Dosovitskiy et al., 2020]

    style Basic fill:#FFE4B5
    style Adv1 fill:#90EE90
    style Adv2 fill:#90EE90
    style Adv3 fill:#90EE90
    style Adv4 fill:#90EE90
    style Adv5 fill:#87CEEB
</div>
<h3>3. 進階正規化技術</h3>
<table>
<thead>
<tr>
<th>技術</th>
<th>原理</th>
<th>效果</th>
<th>適用性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mixup</strong></td>
<td>混合兩張影像和標籤</td>
<td>+2-3%</td>
<td>通用</td>
</tr>
<tr>
<td><strong>Cutout</strong></td>
<td>隨機遮擋區域</td>
<td>+1-2%</td>
<td>影像</td>
</tr>
<tr>
<td><strong>CutMix</strong></td>
<td>剪貼混合</td>
<td>+2-3%</td>
<td>影像</td>
</tr>
<tr>
<td><strong>Label Smoothing</strong></td>
<td>軟化標籤</td>
<td>+0.5-1%</td>
<td>分類</td>
</tr>
<tr>
<td><strong>DropBlock</strong></td>
<td>結構化 Dropout</td>
<td>+1-2%</td>
<td>CNN</td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code><span class="c1"># Mixup 範例</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mixup_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">mixed_x</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="n">y_a</span><span class="p">,</span> <span class="n">y_b</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">mixed_x</span><span class="p">,</span> <span class="n">y_a</span><span class="p">,</span> <span class="n">y_b</span><span class="p">,</span> <span class="n">lam</span>

<span class="c1"># 訓練時使用</span>
<span class="n">mixed_x</span><span class="p">,</span> <span class="n">y_a</span><span class="p">,</span> <span class="n">y_b</span><span class="p">,</span> <span class="n">lam</span> <span class="o">=</span> <span class="n">mixup_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">mixed_x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_a</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_b</span><span class="p">)</span>
</code></pre></div>

<h3>4. 模型壓縮與加速</h3>
<div class="mermaid">
graph TD
    FullModel[完整模型&lt;br/&gt;準確率 87%&lt;br/&gt;100 MB&lt;br/&gt;50 ms] --&gt; Compress{壓縮技術}

    Compress --&gt; Pruning[剪枝&lt;br/&gt;Pruning]
    Compress --&gt; Quantization[量化&lt;br/&gt;Quantization]
    Compress --&gt; Distillation[知識蒸餾&lt;br/&gt;Distillation]

    Pruning --&gt; Result1[準確率 86%&lt;br/&gt;30 MB&lt;br/&gt;20 ms]
    Quantization --&gt; Result2[準確率 86.5%&lt;br/&gt;25 MB&lt;br/&gt;15 ms]
    Distillation --&gt; Result3[準確率 85%&lt;br/&gt;10 MB&lt;br/&gt;10 ms]

    style FullModel fill:#FFE4B5
    style Result1 fill:#90EE90
    style Result2 fill:#90EE90
    style Result3 fill:#90EE90
</div>
<h3>5. 實際應用領域</h3>
<table>
<thead>
<tr>
<th>領域</th>
<th>應用</th>
<th>推薦資料集</th>
<th>難度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>醫療影像</strong></td>
<td>X光診斷、腫瘤偵測</td>
<td>ChestX-ray14, ISIC</td>
<td>⭐⭐⭐⭐☆</td>
</tr>
<tr>
<td><strong>自動駕駛</strong></td>
<td>物體偵測、車道線</td>
<td>KITTI, Cityscapes</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>人臉識別</strong></td>
<td>身份驗證、表情辨識</td>
<td>LFW, CelebA</td>
<td>⭐⭐⭐☆☆</td>
</tr>
<tr>
<td><strong>農業</strong></td>
<td>病蟲害偵測、作物分類</td>
<td>PlantVillage</td>
<td>⭐⭐⭐☆☆</td>
</tr>
<tr>
<td><strong>工業檢測</strong></td>
<td>瑕疵偵測、品質控制</td>
<td>MVTec AD</td>
<td>⭐⭐⭐⭐☆</td>
</tr>
<tr>
<td><strong>零售</strong></td>
<td>商品識別、人流分析</td>
<td>ImageNet, OpenImages</td>
<td>⭐⭐⭐☆☆</td>
</tr>
</tbody>
</table>
<hr />
<h2>完整學習路線圖</h2>
<div class="mermaid">
graph TB
    Start([開始 CNN 之旅]) --&gt; Phase1[階段 1: 基礎&lt;br/&gt;1-2 週]

    Phase1 --&gt; Task1[✓ MNIST + LeNet-5&lt;br/&gt;98%+ 準確率]
    Task1 --&gt; Task2[✓ 理解卷積、池化&lt;br/&gt;手算特徵圖尺寸]
    Task2 --&gt; Task3[✓ Keras 實作&lt;br/&gt;熟悉 Sequential API]

    Task3 --&gt; Phase2[階段 2: 進階&lt;br/&gt;2-3 週]

    Phase2 --&gt; Task4[✓ MNIST + PyTorch&lt;br/&gt;99%+ 準確率]
    Task4 --&gt; Task5[✓ 手寫訓練迴圈&lt;br/&gt;理解 backward]
    Task5 --&gt; Task6[✓ CIFAR-10 挑戰&lt;br/&gt;85%+ 準確率]

    Task6 --&gt; Phase3[階段 3: 深化&lt;br/&gt;3-4 週]

    Phase3 --&gt; Task7[✓ 資料增強技巧&lt;br/&gt;準確率 +5%]
    Task7 --&gt; Task8[✓ Batch Norm, Dropout&lt;br/&gt;理解正規化]
    Task8 --&gt; Task9[✓ 超參數調整&lt;br/&gt;學習率調度]

    Task9 --&gt; Phase4[階段 4: 實戰&lt;br/&gt;4-8 週]

    Phase4 --&gt; Task10[ResNet 實作&lt;br/&gt;殘差連接]
    Task10 --&gt; Task11[遷移學習&lt;br/&gt;ImageNet 預訓練]
    Task11 --&gt; Task12[實際專案&lt;br/&gt;自己的資料集]

    Task12 --&gt; Master([CNN 專家])

    style Start fill:#90EE90
    style Phase1 fill:#FFE4B5
    style Phase2 fill:#FFE4B5
    style Phase3 fill:#FFE4B5
    style Phase4 fill:#FFD700
    style Master fill:#FFD700
</div>
<h3>學習時程建議</h3>
<p><strong>全職學習（每週 40 小時）</strong>：<br />
- 第 1-2 週：完成 MNIST，理解基礎概念<br />
- 第 3-4 週：PyTorch 實作，CIFAR-10 入門<br />
- 第 5-6 週：進階技術，超參數調整<br />
- 第 7-8 週：ResNet、遷移學習<br />
- 第 9-12 週：實際專案，找工作</p>
<p><strong>兼職學習（每週 10 小時）</strong>：<br />
- 第 1-4 週：MNIST 基礎<br />
- 第 5-8 週：PyTorch + CIFAR-10<br />
- 第 9-12 週：進階技術<br />
- 第 13-20 週：ResNet + 專案</p>
<hr />
<h2>CNN 理論與概念完整總結</h2>
<p>本章節系統化地總結所有 CNN 的核心理論，幫助你建立完整的知識體系。</p>
<h3>1. 卷積層的數學原理</h3>
<h4>1.1 卷積運算</h4>
<p><strong>定義</strong>：卷積是一種線性運算，透過滑動濾波器（filter/kernel）在輸入上進行元素乘法和加總。</p>
<p><strong>數學表達式</strong>：</p>
<p>對於 2D 卷積：</p>
<div class="codehilite"><pre><span></span><code>Y[i,j] = Σₘ Σₙ X[i+m, j+n] × K[m,n] + b

其中:
- Y: 輸出特徵圖（Feature Map）
- X: 輸入影像
- K: 卷積核（Kernel）
- b: 偏差項（Bias）
- m,n: 卷積核的索引
</code></pre></div>

<p><strong>實例計算</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 輸入影像 (5×5)</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">]]</span>

<span class="c1"># 卷積核 (3×3) - 邊緣偵測</span>
<span class="n">K</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1"># 計算輸出的一個元素 Y[1,1]（無 padding）:</span>
<span class="n">Y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
         <span class="mi">6</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">7</span><span class="err">×</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="o">+</span> <span class="mi">8</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
         <span class="mi">11</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">12</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">13</span><span class="err">×</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
       <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">-</span><span class="mi">2</span> <span class="o">-</span><span class="mi">3</span> <span class="o">-</span><span class="mi">6</span> <span class="o">+</span><span class="mi">56</span> <span class="o">-</span><span class="mi">8</span> <span class="o">-</span><span class="mi">11</span> <span class="o">-</span><span class="mi">12</span> <span class="o">-</span><span class="mi">13</span>
       <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>

<h4>1.2 輸出尺寸計算</h4>
<p><strong>完整公式</strong>：</p>
<div class="codehilite"><pre><span></span><code>輸出高度 = floor((H + 2P - K) / S) + 1
輸出寬度 = floor((W + 2P - K) / S) + 1

其中:
- H, W: 輸入高度和寬度
- K: 卷積核大小
- P: Padding（填充）
- S: Stride（步幅）
- floor: 向下取整
</code></pre></div>

<p><strong>範例</strong>：</p>
<table>
<thead>
<tr>
<th>輸入</th>
<th>卷積核</th>
<th>Padding</th>
<th>Stride</th>
<th>輸出</th>
<th>計算</th>
</tr>
</thead>
<tbody>
<tr>
<td>32×32</td>
<td>3×3</td>
<td>0</td>
<td>1</td>
<td>30×30</td>
<td>(32-3)/1+1 = 30</td>
</tr>
<tr>
<td>32×32</td>
<td>3×3</td>
<td>1</td>
<td>1</td>
<td>32×32</td>
<td>(32+2-3)/1+1 = 32</td>
</tr>
<tr>
<td>32×32</td>
<td>5×5</td>
<td>2</td>
<td>1</td>
<td>32×32</td>
<td>(32+4-5)/1+1 = 32</td>
</tr>
<tr>
<td>32×32</td>
<td>3×3</td>
<td>0</td>
<td>2</td>
<td>15×15</td>
<td>(32-3)/2+1 = 15</td>
</tr>
</tbody>
</table>
<h4>1.3 參數共享與局部連接</h4>
<p><strong>參數數量對比</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 全連接層（假設輸入 32×32，輸出 64 個神經元）</span>
<span class="n">參數量</span> <span class="o">=</span> <span class="mi">32</span> <span class="err">×</span> <span class="mi">32</span> <span class="err">×</span> <span class="mi">64</span> <span class="o">+</span> <span class="mi">64</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span> <span class="o">=</span> <span class="mi">65</span><span class="p">,</span><span class="mi">600</span>

<span class="c1"># 卷積層（64 個 3×3 濾波器，輸入通道 1）</span>
<span class="n">參數量</span> <span class="o">=</span> <span class="mi">3</span> <span class="err">×</span> <span class="mi">3</span> <span class="err">×</span> <span class="mi">1</span> <span class="err">×</span> <span class="mi">64</span> <span class="o">+</span> <span class="mi">64</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span> <span class="o">=</span> <span class="mi">640</span>

<span class="n">節省</span> <span class="o">=</span> <span class="mi">65</span><span class="p">,</span><span class="mi">600</span> <span class="o">/</span> <span class="mi">640</span> <span class="o">=</span> <span class="mi">102</span> <span class="n">倍</span><span class="err">！</span>
</code></pre></div>

<p><strong>為什麼卷積有效？</strong></p>
<ol>
<li><strong>參數共享</strong>：同一個濾波器掃描整張影像</li>
<li><strong>局部連接</strong>：每個神經元只連接局部區域</li>
<li><strong>平移不變性</strong>：偵測特徵不受位置影響</li>
<li><strong>層次化特徵</strong>：淺層→邊緣，深層→複雜物體</li>
</ol>
<h3>2. 池化層的原理</h3>
<h4>2.1 最大池化（Max Pooling）</h4>
<p><strong>運作機制</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 輸入 (4×4)</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]]</span>

<span class="c1"># Max Pooling 2×2, stride=2</span>
<span class="c1"># 將 4×4 分成 4 個 2×2 區域，取最大值</span>

<span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>  <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">=</span><span class="mi">8</span><span class="p">],</span>
     <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">=</span><span class="mi">4</span><span class="p">]]</span>

<span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
</code></pre></div>

<p><strong>為什麼使用池化？</strong></p>
<table>
<thead>
<tr>
<th>優勢</th>
<th>說明</th>
<th>數值範例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>降維</strong></td>
<td>減少計算量</td>
<td>4×4→2×2，減少 75%</td>
</tr>
<tr>
<td><strong>平移不變性</strong></td>
<td>物體小幅移動不影響輸出</td>
<td>特徵移動 1 像素，池化輸出不變</td>
</tr>
<tr>
<td><strong>抗雜訊</strong></td>
<td>保留最顯著特徵</td>
<td>雜訊通常不是最大值</td>
</tr>
<tr>
<td><strong>擴大感受野</strong></td>
<td>每層看到更大範圍</td>
<td>2 層池化 → 感受野 ×4</td>
</tr>
</tbody>
</table>
<h4>2.2 平均池化（Average Pooling）</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 輸入相同</span>
<span class="c1"># Average Pooling 2×2</span>

<span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="n">avg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span><span class="o">=</span><span class="mf">3.75</span><span class="p">,</span>  <span class="n">avg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">=</span><span class="mf">5.25</span><span class="p">],</span>
     <span class="p">[</span><span class="n">avg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">=</span><span class="mf">2.00</span><span class="p">,</span>  <span class="n">avg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">=</span><span class="mf">2.00</span><span class="p">]]</span>
</code></pre></div>

<p><strong>Max Pooling vs Average Pooling</strong>：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>Max Pooling</th>
<th>Average Pooling</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>用途</strong></td>
<td>特徵偵測（主流）</td>
<td>背景資訊、全局特徵</td>
</tr>
<tr>
<td><strong>保留資訊</strong></td>
<td>最顯著特徵</td>
<td>平均資訊</td>
</tr>
<tr>
<td><strong>現代使用</strong></td>
<td>卷積層之間</td>
<td>全局平均池化（GAP）</td>
</tr>
<tr>
<td><strong>範例</strong></td>
<td>ResNet, VGG</td>
<td>GoogLeNet 最後一層</td>
</tr>
</tbody>
</table>
<h3>3. 激活函數的深度分析</h3>
<h4>3.1 各種激活函數對比</h4>
<div class="mermaid">
graph TB
    subgraph 激活函數特性對比
        A[ReLU&lt;br/&gt;f(x)=max(0,x)] --&gt; A1[優點: 計算快&lt;br/&gt;避免梯度消失]
        A --&gt; A2[缺點: Dead ReLU&lt;br/&gt;負值神經元死亡]

        B[Leaky ReLU&lt;br/&gt;f(x)=max(0.01x,x)] --&gt; B1[優點: 解決 Dead ReLU]
        B --&gt; B2[缺點: 需調整斜率]

        C[Sigmoid&lt;br/&gt;f(x)=1/(1+e⁻ˣ)] --&gt; C1[優點: 平滑、有界]
        C --&gt; C2[缺點: 梯度消失&lt;br/&gt;計算成本高]

        D[Tanh&lt;br/&gt;f(x)=tanh(x)] --&gt; D1[優點: 零中心化&lt;br/&gt;比 Sigmoid 好]
        D --&gt; D2[缺點: 仍有梯度消失]

        E[Softmax&lt;br/&gt;f(xᵢ)=eˣⁱ/Σeˣʲ] --&gt; E1[優點: 輸出機率分布]
        E --&gt; E2[用途: 多分類輸出層]
    end

    style A fill:#90EE90
    style B fill:#90EE90
    style E fill:#FFD700
    style C fill:#FFE4B5
    style D fill:#FFE4B5
</div>
<h4>3.2 激活函數的數學性質</h4>
<table>
<thead>
<tr>
<th>激活函數</th>
<th>數學表達式</th>
<th>導數</th>
<th>值域</th>
<th>優缺點</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ReLU</strong></td>
<td>f(x) = max(0, x)</td>
<td>f'(x) = 1 if x&gt;0 else 0</td>
<td>[0, ∞)</td>
<td>快速、避免梯度消失<br/>❌ Dead ReLU</td>
</tr>
<tr>
<td><strong>Leaky ReLU</strong></td>
<td>f(x) = max(0.01x, x)</td>
<td>f'(x) = 1 if x&gt;0 else 0.01</td>
<td>(-∞, ∞)</td>
<td>解決 Dead ReLU<br/>❌ 需調參</td>
</tr>
<tr>
<td><strong>Sigmoid</strong></td>
<td>f(x) = 1/(1+e⁻ˣ)</td>
<td>f'(x) = f(x)(1-f(x))</td>
<td>(0, 1)</td>
<td>有界、平滑<br/>❌ 梯度消失</td>
</tr>
<tr>
<td><strong>Tanh</strong></td>
<td>f(x) = (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ)</td>
<td>f'(x) = 1 - f(x)²</td>
<td>(-1, 1)</td>
<td>零中心化<br/>❌ 梯度消失</td>
</tr>
</tbody>
</table>
<h4>3.3 Dead ReLU 問題</h4>
<p><strong>什麼是 Dead ReLU？</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 神經元的輸入始終 ≤ 0</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>  <span class="c1"># 負輸入</span>
<span class="n">output</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 輸出永遠是 0</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 梯度永遠是 0</span>

<span class="c1"># 結果：神經元永遠無法更新（「死亡」）</span>
</code></pre></div>

<p><strong>原因</strong>：<br />
1. 不當的權重初始化<br />
2. 學習率過大<br />
3. 輸入資料未正規化</p>
<p><strong>解決方案</strong>：<br />
1. 使用 <strong>Leaky ReLU</strong> 或 <strong>PReLU</strong><br />
2. 正確的權重初始化（He initialization）<br />
3. Batch Normalization<br />
4. 適當的學習率</p>
<h3>4. 正規化技術的理論</h3>
<h4>4.1 Batch Normalization 的數學原理</h4>
<p><strong>算法步驟</strong>：</p>
<p>對於 mini-batch B = {x₁, x₂, ..., xₘ}：</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">計算</span><span class="w"> </span><span class="n">mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="n">均值</span><span class="p">:</span>
<span class="w">   </span><span class="n">μ_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mf">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="n">Σ</span><span class="w"> </span><span class="n">xᵢ</span>

<span class="mf">2.</span><span class="w"> </span><span class="n">計算</span><span class="w"> </span><span class="n">mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="n">方差</span><span class="p">:</span>
<span class="w">   </span><span class="n">σ²_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mf">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="n">Σ</span><span class="w"> </span><span class="p">(</span><span class="n">xᵢ</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">μ_B</span><span class="p">)</span><span class="n">²</span>

<span class="mf">3.</span><span class="w"> </span><span class="n">正規化</span><span class="p">:</span>
<span class="w">   </span><span class="n">x</span><span class="err">̂</span><span class="n">ᵢ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">xᵢ</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">μ_B</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">√</span><span class="p">(</span><span class="n">σ²_B</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ε</span><span class="p">)</span>

<span class="mf">4.</span><span class="w"> </span><span class="n">縮放與平移</span><span class="err">（</span><span class="n">可學習參數</span><span class="w"> </span><span class="n">γ</span><span class="p">,</span><span class="w"> </span><span class="n">β</span><span class="err">）</span><span class="p">:</span>
<span class="w">   </span><span class="n">yᵢ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">γ</span><span class="w"> </span><span class="err">×</span><span class="w"> </span><span class="n">x</span><span class="err">̂</span><span class="n">ᵢ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">β</span>
</code></pre></div>

<p><strong>為什麼有效？</strong></p>
<div class="mermaid">
graph LR
    A[輸入分布不穩定] --&gt; B[內部協變量偏移&lt;br/&gt;Internal Covariate Shift]
    B --&gt; C[每層輸入分布變化]
    C --&gt; D[難以訓練]

    E[Batch Normalization] --&gt; F[穩定輸入分布]
    F --&gt; G[加速收斂]
    F --&gt; H[允許更大學習率]
    F --&gt; I[正規化效果]

    style A fill:#FFB6C1
    style E fill:#90EE90
    style G fill:#FFD700
    style H fill:#FFD700
    style I fill:#FFD700
</div>
<h4>4.2 Dropout 的理論基礎</h4>
<p><strong>運作機制</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 訓練時（Dropout rate = 0.5）</span>
<span class="c1"># 隨機丟棄 50% 的神經元</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># 隨機生成</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># element-wise 乘法</span>

<span class="c1"># 測試時（不使用 Dropout）</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span> <span class="err">×</span> <span class="mf">0.5</span>  <span class="c1"># 縮放補償</span>
</code></pre></div>

<p><strong>為什麼有效？</strong></p>
<ol>
<li><strong>集成學習效果</strong>：</li>
<li>訓練了 2ⁿ 個子網路（n = 神經元數）</li>
<li>
<p>測試時相當於模型平均</p>
</li>
<li>
<p><strong>減少神經元依賴</strong>：</p>
</li>
<li>強迫網路學習冗餘表示</li>
<li>
<p>防止過度依賴特定神經元</p>
</li>
<li>
<p><strong>理論證明</strong>（Srivastava et al., 2014）：</p>
</li>
<li>等價於 L2 正規化的近似</li>
<li>降低模型複雜度</li>
</ol>
<p><strong>Dropout 率選擇</strong>：</p>
<table>
<thead>
<tr>
<th>層類型</th>
<th>推薦 Dropout 率</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>卷積層</strong></td>
<td>0.1-0.3</td>
<td>參數共享已提供正規化</td>
</tr>
<tr>
<td><strong>全連接層</strong></td>
<td>0.5-0.7</td>
<td>參數多，容易過擬合</td>
</tr>
<tr>
<td><strong>輸出層</strong></td>
<td>0</td>
<td>不應隨機丟棄最終預測</td>
</tr>
<tr>
<td><strong>小資料集</strong></td>
<td>0.5-0.7</td>
<td>需要更強正規化</td>
</tr>
<tr>
<td><strong>大資料集</strong></td>
<td>0.2-0.3</td>
<td>資料已提供正規化</td>
</tr>
</tbody>
</table>
<h3>5. 損失函數與優化器</h3>
<h4>5.1 交叉熵損失（Cross-Entropy Loss）</h4>
<p><strong>數學定義</strong>：</p>
<p>對於多分類問題：</p>
<div class="codehilite"><pre><span></span><code>L = -Σᵢ yᵢ × log(ŷᵢ)

其中:
- yᵢ: 真實標籤（One-Hot）
- ŷᵢ: 預測機率（Softmax 輸出）
</code></pre></div>

<p><strong>實例計算</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 真實標籤（類別 2）</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># One-Hot</span>

<span class="c1"># 模型預測（經過 Softmax）</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>

<span class="c1"># 交叉熵損失</span>
<span class="n">Loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">0</span><span class="err">×</span><span class="n">log</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span> <span class="o">+</span> <span class="mi">0</span><span class="err">×</span><span class="n">log</span><span class="p">(</span><span class="mf">0.10</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="err">×</span><span class="n">log</span><span class="p">(</span><span class="mf">0.70</span><span class="p">)</span> <span class="o">+</span> <span class="mi">0</span><span class="err">×</span><span class="n">log</span><span class="p">(</span><span class="mf">0.10</span><span class="p">)</span> <span class="o">+</span> <span class="mi">0</span><span class="err">×</span><span class="n">log</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
     <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.70</span><span class="p">)</span>
     <span class="o">=</span> <span class="mf">0.357</span>

<span class="c1"># 如果預測很準確</span>
<span class="n">y_pred_good</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
<span class="n">Loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.96</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.041</span>  <span class="err">←</span> <span class="n">損失低</span>

<span class="c1"># 如果預測錯誤</span>
<span class="n">y_pred_bad</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
<span class="n">Loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span> <span class="o">=</span> <span class="mf">3.912</span>  <span class="err">←</span> <span class="n">損失高</span>
</code></pre></div>

<p><strong>為什麼使用交叉熵？</strong></p>
<table>
<thead>
<tr>
<th>對比</th>
<th>均方誤差 (MSE)</th>
<th>交叉熵</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>數學形式</strong></td>
<td>Σ(y - ŷ)²</td>
<td>-Σ y log(ŷ)</td>
</tr>
<tr>
<td><strong>梯度特性</strong></td>
<td>飽和時梯度小</td>
<td>錯誤越大梯度越大 ✅</td>
</tr>
<tr>
<td><strong>適用場景</strong></td>
<td>回歸問題</td>
<td>分類問題 ✅</td>
</tr>
<tr>
<td><strong>收斂速度</strong></td>
<td>慢</td>
<td>快 ✅</td>
</tr>
</tbody>
</table>
<h4>5.2 優化器的數學原理</h4>
<p><strong>梯度下降（Gradient Descent）</strong>：</p>
<div class="codehilite"><pre><span></span><code>θₜ₊₁ = θₜ - η × ∇L(θₜ)

其中:
- θ: 參數
- η: 學習率
- ∇L: 損失函數的梯度
</code></pre></div>

<p><strong>動量（Momentum）</strong>：</p>
<div class="codehilite"><pre><span></span><code>vₜ = β × vₜ₋₁ + ∇L(θₜ)
θₜ₊₁ = θₜ - η × vₜ

效果: 累積過去的梯度，加速收斂
</code></pre></div>

<p><strong>Adam (Adaptive Moment Estimation)</strong>：</p>
<div class="codehilite"><pre><span></span><code># 一階動量（梯度的指數移動平均）
mₜ = β₁ × mₜ₋₁ + (1-β₁) × ∇L(θₜ)

# 二階動量（梯度平方的指數移動平均）
vₜ = β₂ × vₜ₋₁ + (1-β₂) × (∇L(θₜ))²

# 偏差修正
m̂ₜ = mₜ / (1-β₁ᵗ)
v̂ₜ = vₜ / (1-β₂ᵗ)

# 參數更新
θₜ₊₁ = θₜ - η × m̂ₜ / (√v̂ₜ + ε)

其中: β₁=0.9, β₂=0.999, ε=10⁻⁸
</code></pre></div>

<p><strong>優化器對比</strong>：</p>
<table>
<thead>
<tr>
<th>優化器</th>
<th>優點</th>
<th>缺點</th>
<th>推薦場景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD</strong></td>
<td>簡單、理論保證</td>
<td>收斂慢、需調參</td>
<td>經典論文複現</td>
</tr>
<tr>
<td><strong>SGD + Momentum</strong></td>
<td>加速、減少震盪</td>
<td>仍需調參</td>
<td>大型資料集</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>自適應、穩健</td>
<td>泛化能力稍差</td>
<td>大多數情況 ✅</td>
</tr>
<tr>
<td><strong>AdamW</strong></td>
<td>Adam + 正確的 L2</td>
<td>略複雜</td>
<td>現代推薦 ✅</td>
</tr>
</tbody>
</table>
<h3>6. 反向傳播的數學原理</h3>
<h4>6.1 鏈式法則（Chain Rule）</h4>
<p>反向傳播的核心是<strong>鏈式法則</strong>：</p>
<div class="codehilite"><pre><span></span><code>對於複合函數 f(g(x))，導數為:
df/dx = (df/dg) × (dg/dx)
</code></pre></div>

<p><strong>CNN 中的應用</strong>：</p>
<div class="codehilite"><pre><span></span><code>損失函數 L 對參數 W 的梯度:

L = Loss(Softmax(Dense(Flatten(Pool(Conv(X, W))))))

∂L/∂W = (∂L/∂Softmax) × (∂Softmax/∂Dense) × ... × (∂Conv/∂W)
</code></pre></div>

<h4>6.2 卷積層的反向傳播</h4>
<p><strong>前向傳播</strong>：</p>
<div class="codehilite"><pre><span></span><code>Y = X ∗ K + b  (∗ 表示卷積)
</code></pre></div>

<p><strong>反向傳播</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">損失對輸出的梯度</span><span class="err">（</span><span class="n">從上一層傳來</span><span class="err">）</span><span class="p">:</span>
<span class="w">   </span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">Y</span>

<span class="mf">2.</span><span class="w"> </span><span class="n">損失對卷積核的梯度</span><span class="p">:</span>
<span class="w">   </span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="err">∗</span><span class="w"> </span><span class="p">(</span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">Y</span><span class="p">)</span>

<span class="mf">3.</span><span class="w"> </span><span class="n">損失對輸入的梯度</span><span class="err">（</span><span class="n">傳給下一層</span><span class="err">）</span><span class="p">:</span>
<span class="w">   </span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">Y</span><span class="p">)</span><span class="w"> </span><span class="err">∗</span><span class="w"> </span><span class="n">K_rotated</span>

<span class="n">其中</span><span class="w"> </span><span class="n">K_rotated</span><span class="w"> </span><span class="n">是</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="n">旋轉</span><span class="w"> </span><span class="mf">180</span><span class="w"> </span><span class="n">度</span>
</code></pre></div>

<p><strong>實例計算</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 前向傳播</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>    <span class="n">K</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>    <span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>         <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>         <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]]</span>

<span class="c1"># 反向傳播（假設 ∂L/∂Y = [[1, 1], [1, 1]]）</span>
<span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">K</span> <span class="o">=</span> <span class="n">X</span> <span class="err">∗</span> <span class="p">(</span><span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">Y</span><span class="p">)</span>
      <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="err">×</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="err">×</span><span class="mi">1</span><span class="o">+</span><span class="mi">3</span><span class="err">×</span><span class="mi">1</span><span class="o">+</span><span class="mi">4</span><span class="err">×</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>  <span class="c1"># 簡化</span>
         <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
      <span class="o">=</span> <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>

<span class="c1"># 卷積核更新</span>
<span class="n">K_new</span> <span class="o">=</span> <span class="n">K</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="err">×</span> <span class="err">∂</span><span class="n">L</span><span class="o">/</span><span class="err">∂</span><span class="n">K</span>
</code></pre></div>

<h3>7. 超參數的系統性影響</h3>
<h4>7.1 學習率的影響</h4>
<div class="mermaid">
graph TD
    Start[模型不收斂] --&gt; Q1{損失是否下降?}

    Q1 --&gt;|完全不降| Path1[檢查基礎設定]
    Q1 --&gt;|下降但停滯| Path2[優化超參數]
    Q1 --&gt;|劇烈震盪| Path3[學習率過大]

    Path1 --&gt; Check1[1. 資料是否正規化?]
    Check1 --&gt; Check2[2. 標籤格式正確?]
    Check2 --&gt; Check3[3. 優化器設定正確?]
    Check3 --&gt; Check4[4. 損失函數適合?]

    Path2 --&gt; Opt1[1. 提高學習率]
    Opt1 --&gt; Opt2[2. 增加模型容量]
    Opt2 --&gt; Opt3[3. 檢查資料品質]

    Path3 --&gt; Fix1[降低學習率&lt;br/&gt;0.001 → 0.0001]

    Check1 --&gt; Fix2[正規化到 [0,1]&lt;br/&gt;或標準化]
    Check2 --&gt; Fix3[Keras: One-Hot&lt;br/&gt;PyTorch: 類別索引]
    Check3 --&gt; Fix4[檢查 zero_grad&lt;br/&gt;檢查 optimizer.step]
    Check4 --&gt; Fix5[分類: CrossEntropy&lt;br/&gt;回歸: MSE]

    style Start fill:#FFB6C1
    style Fix2 fill:#90EE90
    style Fix3 fill:#90EE90
    style Fix4 fill:#90EE90
    style Fix5 fill:#90EE90
    style Fix1 fill:#90EE90
</div>0</p>
<p><strong>數學分析</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="err">梯度下降</span><span class="o">:</span><span class="w"> </span><span class="err">θₜ₊₁</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">θₜ</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">η</span><span class="w"> </span><span class="err">×</span><span class="w"> </span><span class="err">∇</span><span class="n">L</span><span class="o">(</span><span class="err">θₜ</span><span class="o">)</span>

<span class="err">η</span><span class="w"> </span><span class="err">過大</span><span class="o">:</span>
<span class="o">-</span><span class="w"> </span><span class="err">步長太大，跨過最優點</span>
<span class="o">-</span><span class="w"> </span><span class="err">損失函數值劇烈震盪</span>
<span class="o">-</span><span class="w"> </span><span class="n">Loss</span><span class="o">:</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">1.2</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.3</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">2.1</span><span class="w"> </span><span class="o">(</span><span class="err">不穩定</span><span class="o">)</span>

<span class="err">η</span><span class="w"> </span><span class="err">適中</span><span class="o">:</span>
<span class="o">-</span><span class="w"> </span><span class="err">穩步向最優點前進</span>
<span class="o">-</span><span class="w"> </span><span class="n">Loss</span><span class="o">:</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.4</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.3</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.2</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.15</span><span class="w"> </span><span class="o">(</span><span class="err">穩定下降</span><span class="o">)</span>

<span class="err">η</span><span class="w"> </span><span class="err">過小</span><span class="o">:</span>
<span class="o">-</span><span class="w"> </span><span class="err">步長太小，移動緩慢</span>
<span class="o">-</span><span class="w"> </span><span class="n">Loss</span><span class="o">:</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.499</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="mf">0.498</span><span class="w"> </span><span class="o">(</span><span class="err">幾乎不動</span><span class="o">)</span>
</code></pre></div>

<h4>7.2 Batch Size 的理論影響</h4>
<p><strong>數學視角</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="err">梯度估計的方差</span><span class="o">:</span>
<span class="n">Var</span><span class="o">(</span><span class="err">∇</span><span class="n">L</span><span class="o">)</span><span class="w"> </span><span class="err">∝</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">batch_size</span>

<span class="n">Batch</span><span class="w"> </span><span class="n">Size</span><span class="w"> </span><span class="err">↑</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">Var</span><span class="w"> </span><span class="err">↓</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">梯度估計更準確</span>
<span class="n">Batch</span><span class="w"> </span><span class="n">Size</span><span class="w"> </span><span class="err">↓</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">Var</span><span class="w"> </span><span class="err">↑</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">更多隨機性（有時是好事）</span>
</code></pre></div>

<p><strong>實際影響</strong>：</p>
<table>
<thead>
<tr>
<th>Batch Size</th>
<th>梯度準確性</th>
<th>泛化能力</th>
<th>訓練速度</th>
<th>記憶體</th>
<th>推薦</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>32</strong></td>
<td>低</td>
<td>很好</td>
<td>慢</td>
<td>低</td>
<td>小資料集</td>
</tr>
<tr>
<td><strong>64</strong></td>
<td>中</td>
<td>好</td>
<td>中</td>
<td>中</td>
<td>平衡選擇</td>
</tr>
<tr>
<td><strong>128</strong></td>
<td>高</td>
<td>好</td>
<td>快</td>
<td>中高</td>
<td>推薦 ✅</td>
</tr>
<tr>
<td><strong>256</strong></td>
<td>很高</td>
<td>中</td>
<td>很快</td>
<td>高</td>
<td>大資料集</td>
</tr>
<tr>
<td><strong>512+</strong></td>
<td>極高</td>
<td>差</td>
<td>極快</td>
<td>很高</td>
<td>需特殊調整</td>
</tr>
</tbody>
</table>
<p><strong>大 Batch Size 的調整</strong>：</p>
<p>根據線性縮放規則（Linear Scaling Rule, Goyal et al., 2017）：</p>
<div class="codehilite"><pre><span></span><code>如果 Batch Size 增加 k 倍
則 Learning Rate 也應增加 k 倍

範例:
Batch Size 128, LR = 0.001
Batch Size 256, LR = 0.002  ← 2 倍
Batch Size 512, LR = 0.004  ← 4 倍
</code></pre></div>

<h3>8. 過擬合與欠擬合的理論本質</h3>
<h4>8.1 偏差-方差權衡（Bias-Variance Tradeoff）</h4>
<p><strong>數學定義</strong>：</p>
<div class="codehilite"><pre><span></span><code>預期誤差 = Bias² + Variance + 不可約誤差

其中:
- Bias (偏差): 模型的平均預測與真實值的差距
- Variance (方差): 模型對訓練資料的敏感程度
- 不可約誤差: 資料本身的雜訊
</code></pre></div>

<div class="mermaid">
graph TD
    Start[模型不收斂] --&gt; Q1{損失是否下降?}

    Q1 --&gt;|完全不降| Path1[檢查基礎設定]
    Q1 --&gt;|下降但停滯| Path2[優化超參數]
    Q1 --&gt;|劇烈震盪| Path3[學習率過大]

    Path1 --&gt; Check1[1. 資料是否正規化?]
    Check1 --&gt; Check2[2. 標籤格式正確?]
    Check2 --&gt; Check3[3. 優化器設定正確?]
    Check3 --&gt; Check4[4. 損失函數適合?]

    Path2 --&gt; Opt1[1. 提高學習率]
    Opt1 --&gt; Opt2[2. 增加模型容量]
    Opt2 --&gt; Opt3[3. 檢查資料品質]

    Path3 --&gt; Fix1[降低學習率&lt;br/&gt;0.001 → 0.0001]

    Check1 --&gt; Fix2[正規化到 [0,1]&lt;br/&gt;或標準化]
    Check2 --&gt; Fix3[Keras: One-Hot&lt;br/&gt;PyTorch: 類別索引]
    Check3 --&gt; Fix4[檢查 zero_grad&lt;br/&gt;檢查 optimizer.step]
    Check4 --&gt; Fix5[分類: CrossEntropy&lt;br/&gt;回歸: MSE]

    style Start fill:#FFB6C1
    style Fix2 fill:#90EE90
    style Fix3 fill:#90EE90
    style Fix4 fill:#90EE90
    style Fix5 fill:#90EE90
    style Fix1 fill:#90EE90
</div>1</p>
<h4>8.2 模型容量理論</h4>
<p><strong>VC 維（Vapnik-Chervonenkis Dimension）</strong>：</p>
<p>模型能夠擬合的最大樣本數量</p>
<div class="codehilite"><pre><span></span><code>線性模型（2D）: VC 維 = 3
  - 可以完美分類任意 3 個點
  - 無法保證分類 4 個點

深度神經網路: VC 維 ≈ O(W × log W)
  - W: 參數總數
  - 容量極大，容易過擬合
</code></pre></div>

<p><strong>正規化的理論作用</strong>：</p>
<div class="codehilite"><pre><span></span><code>優化目標（無正規化）:
min L(θ) = Σᵢ Loss(f(xᵢ; θ), yᵢ)

優化目標（有正規化）:
min L(θ) + λ × R(θ)

其中:
- R(θ): 正規化項（如 ||θ||²）
- λ: 正規化強度

效果: 限制模型容量，防止過擬合
</code></pre></div>

<h3>9. 深度學習的理論基礎</h3>
<h4>9.1 萬有近似定理（Universal Approximation Theorem）</h4>
<p><strong>定理內容</strong>：</p>
<p>具有單個隱藏層的神經網路，只要有足夠多的神經元，就可以以任意精度近似任何連續函數。</p>
<p><strong>數學表達</strong>：</p>
<div class="codehilite"><pre><span></span><code>對於任意連續函數 f: [0,1]ⁿ → ℝ
存在神經網路 N(x)，使得:
|N(x) - f(x)| &lt; ε  (對所有 x)

其中 ε 是任意小的正數
</code></pre></div>

<p><strong>實際意義</strong>：</p>
<p>✅ <strong>理論上</strong>：單層網路可以表示任何函數<br />
❌ <strong>實際上</strong>：<br />
- 需要指數級數量的神經元<br />
- 訓練極其困難<br />
- 深層網路更高效</p>
<h4>9.2 為什麼深度重要？</h4>
<p><strong>表示能力的指數增長</strong>：</p>
<div class="codehilite"><pre><span></span><code>淺層網路（1 層隱藏層）:
表示能力 ∝ O(n)  (n: 神經元數)

深層網路（L 層）:
表示能力 ∝ O(nᴸ)  (指數增長！)
</code></pre></div>

<p><strong>層次化特徵學習</strong>：</p>
<div class="codehilite"><pre><span></span><code>輸入影像
    ↓
第 1 層: 邊緣、角點
    ↓
第 2 層: 簡單形狀（圓形、方形）
    ↓
第 3 層: 物體部件（眼睛、鼻子、輪胎）
    ↓
第 4 層: 完整物體（臉、汽車）
    ↓
輸出: 分類結果
</code></pre></div>

<h3>10. CNN 的數學性質總結</h3>
<h4>10.1 平移不變性（Translation Invariance）</h4>
<p><strong>定義</strong>：物體在影像中的位置改變，CNN 的輸出不變（或變化很小）</p>
<p><strong>實現機制</strong>：<br />
1. <strong>參數共享</strong>：同一個濾波器掃描整張影像<br />
2. <strong>池化</strong>：降低空間解析度<br />
3. <strong>大感受野</strong>：深層神經元看到大範圍</p>
<p><strong>數學表達</strong>：</p>
<div class="codehilite"><pre><span></span><code>對於平移 τ:
f(T_τ(x)) ≈ f(x)

其中 T_τ 是平移操作
</code></pre></div>

<h4>10.2 局部連接與稀疏交互</h4>
<p><strong>傳統神經網路</strong>：</p>
<div class="codehilite"><pre><span></span><code>每個神經元與所有輸入連接
參數量 = 輸入數 × 輸出數
</code></pre></div>

<p><strong>CNN</strong>：</p>
<div class="codehilite"><pre><span></span><code>每個神經元只與局部感受野連接
參數量 = 卷積核大小 × 濾波器數

節省參數 = 1 - (K²/H×W) ≈ 99%
(K=3, H=W=32 的情況)
</code></pre></div>

<h4>10.3 感受野的遞增</h4>
<p><strong>計算公式</strong>：</p>
<div class="codehilite"><pre><span></span><code>第 l 層的感受野:
RF_l = RF_{l-1} + (K_l - 1) × Π_{i=1}^{l-1} S_i

其中:
<span class="k">-</span> K_l: 第 l 層的卷積核大小
<span class="k">-</span> S_i: 第 i 層的步幅
</code></pre></div>

<p><strong>實例</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="err">輸入</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="err">×</span><span class="mi">32</span>
<span class="n">Conv1</span><span class="w"> </span><span class="o">(</span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="o">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">RF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
<span class="n">Pool1</span><span class="w"> </span><span class="o">(</span><span class="mi">2</span><span class="err">×</span><span class="mi">2</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="o">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">RF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">6</span>
<span class="n">Conv2</span><span class="w"> </span><span class="o">(</span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="o">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">RF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span>
<span class="n">Pool2</span><span class="w"> </span><span class="o">(</span><span class="mi">2</span><span class="err">×</span><span class="mi">2</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="o">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">RF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">20</span>
<span class="n">Conv3</span><span class="w"> </span><span class="o">(</span><span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="o">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">RF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">28</span>

<span class="err">最終感受野幾乎覆蓋整張影像！</span>
</code></pre></div>

<h3>理論總結的關鍵要點</h3>
<div class="mermaid">
graph TD
    Start[模型不收斂] --&gt; Q1{損失是否下降?}

    Q1 --&gt;|完全不降| Path1[檢查基礎設定]
    Q1 --&gt;|下降但停滯| Path2[優化超參數]
    Q1 --&gt;|劇烈震盪| Path3[學習率過大]

    Path1 --&gt; Check1[1. 資料是否正規化?]
    Check1 --&gt; Check2[2. 標籤格式正確?]
    Check2 --&gt; Check3[3. 優化器設定正確?]
    Check3 --&gt; Check4[4. 損失函數適合?]

    Path2 --&gt; Opt1[1. 提高學習率]
    Opt1 --&gt; Opt2[2. 增加模型容量]
    Opt2 --&gt; Opt3[3. 檢查資料品質]

    Path3 --&gt; Fix1[降低學習率&lt;br/&gt;0.001 → 0.0001]

    Check1 --&gt; Fix2[正規化到 [0,1]&lt;br/&gt;或標準化]
    Check2 --&gt; Fix3[Keras: One-Hot&lt;br/&gt;PyTorch: 類別索引]
    Check3 --&gt; Fix4[檢查 zero_grad&lt;br/&gt;檢查 optimizer.step]
    Check4 --&gt; Fix5[分類: CrossEntropy&lt;br/&gt;回歸: MSE]

    style Start fill:#FFB6C1
    style Fix2 fill:#90EE90
    style Fix3 fill:#90EE90
    style Fix4 fill:#90EE90
    style Fix5 fill:#90EE90
    style Fix1 fill:#90EE90
</div>2</p>
<p><strong>核心公式速查表</strong>：</p>
<table>
<thead>
<tr>
<th>概念</th>
<th>公式</th>
<th>應用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>卷積輸出</strong></td>
<td>H_out = (H + 2P - K)/S + 1</td>
<td>計算特徵圖尺寸</td>
</tr>
<tr>
<td><strong>參數數量</strong></td>
<td>params = K×K×C_in×C_out + C_out</td>
<td>估算模型大小</td>
</tr>
<tr>
<td><strong>感受野</strong></td>
<td>RF_l = RF_{l-1} + (K_l-1)×∏S_i</td>
<td>設計網路深度</td>
</tr>
<tr>
<td><strong>交叉熵</strong></td>
<td>L = -Σ y_i log(ŷ_i)</td>
<td>分類損失</td>
</tr>
<tr>
<td><strong>Adam 更新</strong></td>
<td>θ = θ - η×m̂/√v̂</td>
<td>參數優化</td>
</tr>
<tr>
<td><strong>Batch Norm</strong></td>
<td>x̂ = (x-μ)/√(σ²+ε)</td>
<td>穩定訓練</td>
</tr>
</tbody>
</table>
<hr />
<h2>總結與下一步</h2>
<h3>你已經學會了什麼</h3>
<p>✅ <strong>基礎概念</strong>：<br />
- 卷積、池化、全連接層的原理<br />
- 前向傳播與反向傳播<br />
- 損失函數與優化器</p>
<p>✅ <strong>兩大框架</strong>：<br />
- Keras：快速原型開發<br />
- PyTorch：深入理解原理</p>
<p>✅ <strong>兩個經典任務</strong>：<br />
- MNIST：灰階影像，98%+ 準確率<br />
- CIFAR-10：彩色影像，85%+ 準確率</p>
<p>✅ <strong>核心技術</strong>：<br />
- 資料增強<br />
- Batch Normalization<br />
- Dropout<br />
- 學習率調度</p>
<p>✅ <strong>實戰技巧</strong>：<br />
- 診斷模型問題<br />
- 超參數調整<br />
- 效能優化</p>
<h3>推薦學習資源</h3>
<h4>線上課程</h4>
<ol>
<li><strong>Coursera: Deep Learning Specialization (Andrew Ng)</strong></li>
<li>系統化學習深度學習</li>
<li>
<p>包含 CNN 專題</p>
</li>
<li>
<p><strong>Fast.ai: Practical Deep Learning</strong></p>
</li>
<li>實戰導向</li>
<li>
<p>從上到下的學習方式</p>
</li>
<li>
<p><strong>Stanford CS231n: Convolutional Neural Networks</strong></p>
</li>
<li>斯坦福經典課程</li>
<li>理論深入</li>
</ol>
<h4>論文閱讀</h4>
<p>按順序閱讀：</p>
<ol>
<li><strong>LeNet-5</strong> (1998) - 基礎</li>
<li>
<p>"Gradient-Based Learning Applied to Document Recognition"</p>
</li>
<li>
<p><strong>AlexNet</strong> (2012) - 深度學習革命</p>
</li>
<li>
<p>"ImageNet Classification with Deep CNNs"</p>
</li>
<li>
<p><strong>VGGNet</strong> (2014) - 更深的網路</p>
</li>
<li>
<p>"Very Deep Convolutional Networks"</p>
</li>
<li>
<p><strong>ResNet</strong> (2015) - 殘差連接</p>
</li>
<li>
<p>"Deep Residual Learning for Image Recognition"</p>
</li>
<li>
<p><strong>EfficientNet</strong> (2019) - 模型縮放</p>
</li>
<li>"EfficientNet: Rethinking Model Scaling"</li>
</ol>
<h4>實戰專案建議</h4>
<ol>
<li><strong>初級</strong>：</li>
<li>Fashion-MNIST 分類</li>
<li>Dogs vs Cats 分類</li>
<li>
<p>手寫中文字辨識</p>
</li>
<li>
<p><strong>中級</strong>：</p>
</li>
<li>CIFAR-100 (100 類別)</li>
<li>Tiny ImageNet</li>
<li>
<p>自己收集資料集</p>
</li>
<li>
<p><strong>高級</strong>：</p>
</li>
<li>Kaggle 競賽</li>
<li>醫療影像診斷</li>
<li>自動駕駛物體偵測</li>
</ol>
<h3>下一步行動計畫</h3>
<p><strong>本週</strong>：<br />
- [ ] 完整執行本指南所有程式碼<br />
- [ ] 在 MNIST 達到 99%+ 準確率<br />
- [ ] 在 CIFAR-10 達到 85%+ 準確率</p>
<p><strong>下個月</strong>：<br />
- [ ] 實作 ResNet-18<br />
- [ ] 嘗試遷移學習<br />
- [ ] 完成一個小型專案</p>
<p><strong>三個月內</strong>：<br />
- [ ] 參加一次 Kaggle 競賽<br />
- [ ] 閱讀 5 篇經典論文<br />
- [ ] 建立個人 GitHub 作品集</p>
<hr />
<h2>附錄：完整程式碼索引</h2>
<table>
<thead>
<tr>
<th>檔案</th>
<th>內容</th>
<th>框架</th>
<th>難度</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CNN_intro_b07.md</code></td>
<td>基礎知識、MNIST + LeNet-5</td>
<td>Keras</td>
<td>★☆☆☆☆</td>
</tr>
<tr>
<td><code>CNN_intro_b07_part2.md</code></td>
<td>MNIST + SimpleCNN</td>
<td>PyTorch</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td><code>CNN_intro_b07_part3.md</code></td>
<td>CIFAR-10 完整實戰</td>
<td>Both</td>
<td>★★★★☆</td>
</tr>
<tr>
<td><code>CNN_intro_b07_part4.md</code></td>
<td>實戰技巧與總結</td>
<td>Both</td>
<td>★★★★★</td>
</tr>
</tbody>
</table>
<h3>快速導航</h3>
<p><strong>我想...</strong></p>
<ul>
<li><a href="#">了解 CNN 基礎概念</a> → Part 1</li>
<li><a href="#">快速實作 MNIST</a> → Part 1 (Keras) 或 Part 2 (PyTorch)</li>
<li><a href="#">學習 PyTorch</a> → Part 2</li>
<li><a href="#">挑戰 CIFAR-10</a> → Part 3</li>
<li><a href="#">優化模型效能</a> → Part 4</li>
<li><a href="#">解決訓練問題</a> → Part 4 (疑難排解)</li>
</ul>
<hr />
<p><strong>本文件完成時間</strong>：2025-10-07 15:30:00<br />
<strong>版本</strong>：b07_part4<br />
<strong>系列完成</strong>：全四部分已完成 ✅</p>
<p><strong>感謝閱讀！祝你學習愉快！</strong> 🎉</p></div>
    
    <!-- Highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <!-- Mermaid -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({
            "startOnLoad": true,
            "theme": "dark",
            "securityLevel": "loose"
});
    </script>
</body>
</html>